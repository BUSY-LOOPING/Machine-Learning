{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718c0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727fd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b472e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(N, D)\n",
    "half = int(N/2)\n",
    "\n",
    "X[:half, :] = X[:half, :] - 2 * np.ones((half, D))\n",
    "X[half:, :] = X[half:, :] + 2 * np.ones((half, D))\n",
    "\n",
    "ones = np.ones((N, 1), dtype='uint8')\n",
    "Xb = np.concatenate((ones, X), axis = 1)\n",
    "T = np.array([0] * half + [1] * half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e8c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a) :\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def cross_entropy(T, Y) :\n",
    "    E = 0\n",
    "    for i in range(len(T)) : \n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else :\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f17e006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross entropy is :  196.94934793695833\n"
     ]
    }
   ],
   "source": [
    "#randomly initalizing the weights\n",
    "w = np.random.randn(D + 1) / np.sqrt(D + 1)\n",
    "\n",
    "#calculate the model output \n",
    "z = Xb.dot(w)\n",
    "\n",
    "#apply the activation function\n",
    "Y = sigmoid(z)\n",
    "\n",
    "print('The cross entropy is : ', cross_entropy(T, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15537f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross entropy is 196.94934793695833\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 0.0\n",
      "The cross entropy is 4.440892098500627e-16\n",
      "The cross entropy is 1.4654943925052174e-14\n",
      "The cross entropy is 3.542721671579502e-13\n",
      "The cross entropy is 6.228240145864061e-12\n",
      "The cross entropy is 8.22106827092426e-11\n",
      "The cross entropy is 8.383710396093109e-10\n",
      "The cross entropy is 6.77790748048101e-09\n",
      "The cross entropy is 4.4462810422670795e-08\n",
      "The cross entropy is 2.4167520307021897e-07\n",
      "The cross entropy is 1.1091974431392457e-06\n",
      "The cross entropy is 4.372867201934492e-06\n",
      "The cross entropy is 1.5041716752912997e-05\n",
      "The cross entropy is 4.580167569874e-05\n",
      "The cross entropy is 0.0001251474045534592\n",
      "The cross entropy is 0.0003108690662443645\n",
      "The cross entropy is 0.0007109760731981211\n",
      "The cross entropy is 0.00151571491195329\n",
      "The cross entropy is 0.0030475607111927626\n",
      "The cross entropy is 0.005839196478283473\n",
      "The cross entropy is 0.010745331410060122\n",
      "The cross entropy is 0.019065206834165117\n",
      "The cross entropy is 0.032565431242575445\n",
      "The cross entropy is 0.05311433287575944\n",
      "The cross entropy is 0.081476324408118\n",
      "The cross entropy is 0.11529065740082448\n",
      "The cross entropy is 0.14822038687013267\n",
      "The cross entropy is 0.17332377413287564\n",
      "The cross entropy is 0.1882674862550978\n",
      "The cross entropy is 0.19551602617149283\n",
      "The cross entropy is 0.19854199793285837\n",
      "The cross entropy is 0.1996515674207971\n",
      "The cross entropy is 0.1999844109346617\n",
      "The final weight is :  [-0.20796348  2.17406973  2.18343467]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp/ipykernel_3400/3877726108.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-a))\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "l2 = 0.1\n",
    "\n",
    "for i in range(500):\n",
    "    if i % 10 == 0:\n",
    "        print('The cross entropy is', cross_entropy(T, Y))\n",
    "        \n",
    "        # gradient descent weight update with l2 regularization\n",
    "        w -= learning_rate * (np.dot((Y - T).T, Xb) + l2 * w)\n",
    "        \n",
    "        Y = sigmoid(Xb.dot(w))\n",
    "        \n",
    "print('The final weight is : ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
