{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6088d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "#import seaborn as sns\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from numba import cuda \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29af0262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>5-263831-B-6.wav</td>\n",
       "      <td>6</td>\n",
       "      <td>hen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>5-263902-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>5-51149-A-25.wav</td>\n",
       "      <td>25</td>\n",
       "      <td>footsteps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>5-61635-A-8.wav</td>\n",
       "      <td>8</td>\n",
       "      <td>sheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  target        category\n",
       "0      1-100032-A-0.wav       0             dog\n",
       "1     1-100038-A-14.wav      14  chirping_birds\n",
       "2     1-100210-A-36.wav      36  vacuum_cleaner\n",
       "3     1-100210-B-36.wav      36  vacuum_cleaner\n",
       "4     1-101296-A-19.wav      19    thunderstorm\n",
       "...                 ...     ...             ...\n",
       "1995   5-263831-B-6.wav       6             hen\n",
       "1996  5-263902-A-36.wav      36  vacuum_cleaner\n",
       "1997   5-51149-A-25.wav      25       footsteps\n",
       "1998    5-61635-A-8.wav       8           sheep\n",
       "1999     5-9032-A-0.wav       0             dog\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files_path = fr'E:\\Downloads\\audio-dataset'\n",
    "df = pd.read_csv(audio_files_path + '\\esc50.csv')\n",
    "df = df.drop(['fold', 'esc10', 'src_file', 'take'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13baf193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files = np.array(glob(audio_files_path + '\\*\\*.wav'))\n",
    "audio_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5650bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spectogram(y_tuple) :\n",
    "    D = librosa.stft(y_tuple[0])\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref = np.max)  #sound in decibel\n",
    "    return S_db\n",
    "\n",
    "def to_mel_spectogram(y_tuple) :\n",
    "    S = librosa.feature.melspectrogram(y = y_tuple[0], sr = y_tuple[1], n_mels=128)\n",
    "    return S\n",
    "\n",
    "def to_mfcc(y_tuple) :\n",
    "    mfccs_features = librosa.feature.mfcc(y = y_tuple[0], sr = y_tuple[1], n_mfcc=128)\n",
    "    #in order to find out scaled feature we do mean of transpose of value\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "#     return mfccs_features\n",
    "    return mfccs_scaled_features\n",
    "\n",
    "def get_data(start = 0, stop = 11) :\n",
    "    audio_files_df = pd.DataFrame(audio_files, columns=['full_path'])\n",
    "    audio_files_df['filename'] = audio_files_df.iloc[:, 0].map(lambda x : os.path.basename(x))\n",
    "    \n",
    "    new_df = audio_files_df.merge(df).iloc[start:stop] \n",
    "    del audio_files_df\n",
    "        \n",
    "    new_df['raw_data_sr'] = new_df.loc[:, 'full_path'].map(lambda x : librosa.load(x))\n",
    "#     new_df['spectrogram'] = new_df.loc[:, 'raw_data_sr'].map(to_spectogram)\n",
    "#     new_df['mel_spectrogram'] = new_df.loc[:, 'raw_data_sr'].map(to_mel_spectogram)\n",
    "    new_df['mfcc_scaled'] = new_df.loc[:, 'raw_data_sr'].map(to_mfcc)\n",
    "\n",
    "    X = np.stack(new_df['mfcc_scaled'].values)\n",
    "    print(X)\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    #T = pd.Categorical(new_df['category']).codes\n",
    "    T = new_df.loc[:, 'target'].values.astype(np.int16)\n",
    "    del new_df\n",
    "    return X, T\n",
    "\n",
    "def release_mem() :\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f599d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xorig = np.load('Xorig.npy')\n",
    "Torig = np.load('Torig.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2d42724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.0109698e+02  4.9158282e+00 -8.7268047e+00 ...  4.2269476e-02\n",
      "  -1.2121227e-02  3.4730271e-02]\n",
      " [-1.9649599e+02  6.3035016e+00 -6.2931282e+01 ...  1.5449895e-01\n",
      "  -8.0165707e-02  8.5282095e-02]\n",
      " [ 1.2359730e+01  6.2431587e+01 -1.4457329e+01 ...  4.0643170e-01\n",
      "  -1.2845746e-01  5.7990426e-01]\n",
      " ...\n",
      " [-2.8908960e+02  8.4523781e+01  8.0870180e+00 ... -4.0449437e-02\n",
      "  -1.5515727e-01 -1.7345512e-01]\n",
      " [-1.5004889e+02  9.7708214e+01 -4.0430267e+01 ...  5.6451374e-01\n",
      "   4.7682995e-01 -1.9385140e-01]\n",
      " [-4.9152194e+02  5.9743629e+00 -3.0747921e+01 ...  1.8151146e-01\n",
      "   2.4784334e-01  2.8620649e-02]]\n"
     ]
    }
   ],
   "source": [
    "Xorig, Torig = get_data(0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Xorig', Xorig)\n",
    "np.save('Torig', Torig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6fdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b4f40",
   "metadata": {},
   "source": [
    "### Getting the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de13aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release_mem()\n",
    "Xorig, Torig = shuffle(Xorig, Torig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb101120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xorig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a59124ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release_mem()\n",
    "with tf.device('/cpu:0'):\n",
    "    model=Sequential()\n",
    "    ###1st layer\n",
    "    model.add(Dense(512 ,input_shape=(Xorig.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    ###2nd layer\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    ###final layer\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c809a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_51 (Dense)            (None, 512)               66048     \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " activation_52 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_53 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 50)                3250      \n",
      "                                                                 \n",
      " activation_54 (Activation)  (None, 50)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,218\n",
      "Trainable params: 143,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebc188",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7aba59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(learning_rate=0.01)\n",
    "optimizer_adam = keras.optimizers.Adam(learning_rate=0.001) \n",
    "\n",
    "model.compile(optimizer=optimizer_adam,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23006af1",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "660f9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 3.9989 - accuracy: 0.0212\n",
      "Epoch 1: val_loss improved from inf to 3.89627, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 3.9946 - accuracy: 0.0205 - val_loss: 3.8963 - val_accuracy: 0.0300\n",
      "Epoch 2/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 3.9019 - accuracy: 0.0259\n",
      "Epoch 2: val_loss improved from 3.89627 to 3.86688, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.9018 - accuracy: 0.0253 - val_loss: 3.8669 - val_accuracy: 0.0500\n",
      "Epoch 3/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 3.8427 - accuracy: 0.0367\n",
      "Epoch 3: val_loss improved from 3.86688 to 3.78050, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.8403 - accuracy: 0.0379 - val_loss: 3.7805 - val_accuracy: 0.0200\n",
      "Epoch 4/500\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 3.7978 - accuracy: 0.0538\n",
      "Epoch 4: val_loss improved from 3.78050 to 3.65355, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 3.7917 - accuracy: 0.0542 - val_loss: 3.6536 - val_accuracy: 0.0800\n",
      "Epoch 5/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 3.6855 - accuracy: 0.0563\n",
      "Epoch 5: val_loss improved from 3.65355 to 3.54958, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.6855 - accuracy: 0.0563 - val_loss: 3.5496 - val_accuracy: 0.0400\n",
      "Epoch 6/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 3.5790 - accuracy: 0.0715\n",
      "Epoch 6: val_loss improved from 3.54958 to 3.46440, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.5763 - accuracy: 0.0732 - val_loss: 3.4644 - val_accuracy: 0.0700\n",
      "Epoch 7/500\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 3.5402 - accuracy: 0.0770\n",
      "Epoch 7: val_loss improved from 3.46440 to 3.37924, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.5318 - accuracy: 0.0774 - val_loss: 3.3792 - val_accuracy: 0.0400\n",
      "Epoch 8/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 3.4490 - accuracy: 0.0937\n",
      "Epoch 8: val_loss improved from 3.37924 to 3.30407, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 3.4490 - accuracy: 0.0937 - val_loss: 3.3041 - val_accuracy: 0.0800\n",
      "Epoch 9/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 3.4218 - accuracy: 0.0855\n",
      "Epoch 9: val_loss improved from 3.30407 to 3.23363, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 3.4104 - accuracy: 0.0879 - val_loss: 3.2336 - val_accuracy: 0.1100\n",
      "Epoch 10/500\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 3.3119 - accuracy: 0.1186\n",
      "Epoch 10: val_loss improved from 3.23363 to 3.18749, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.3085 - accuracy: 0.1189 - val_loss: 3.1875 - val_accuracy: 0.1000\n",
      "Epoch 11/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 3.2418 - accuracy: 0.1156\n",
      "Epoch 11: val_loss improved from 3.18749 to 3.16394, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.2497 - accuracy: 0.1153 - val_loss: 3.1639 - val_accuracy: 0.1100\n",
      "Epoch 12/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 3.2093 - accuracy: 0.1207\n",
      "Epoch 12: val_loss improved from 3.16394 to 3.09063, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.2039 - accuracy: 0.1211 - val_loss: 3.0906 - val_accuracy: 0.1500\n",
      "Epoch 13/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 3.1562 - accuracy: 0.1286\n",
      "Epoch 13: val_loss improved from 3.09063 to 3.07258, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.1568 - accuracy: 0.1300 - val_loss: 3.0726 - val_accuracy: 0.1600\n",
      "Epoch 14/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 3.0967 - accuracy: 0.1501\n",
      "Epoch 14: val_loss improved from 3.07258 to 3.00551, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.0972 - accuracy: 0.1505 - val_loss: 3.0055 - val_accuracy: 0.1700\n",
      "Epoch 15/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 3.0430 - accuracy: 0.1489\n",
      "Epoch 15: val_loss improved from 3.00551 to 2.94546, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.0445 - accuracy: 0.1468 - val_loss: 2.9455 - val_accuracy: 0.2000\n",
      "Epoch 16/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 3.0166 - accuracy: 0.1611\n",
      "Epoch 16: val_loss improved from 2.94546 to 2.92156, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.9972 - accuracy: 0.1621 - val_loss: 2.9216 - val_accuracy: 0.1800\n",
      "Epoch 17/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 2.9453 - accuracy: 0.1755\n",
      "Epoch 17: val_loss improved from 2.92156 to 2.90033, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.9510 - accuracy: 0.1726 - val_loss: 2.9003 - val_accuracy: 0.1900\n",
      "Epoch 18/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 2.9838 - accuracy: 0.1759\n",
      "Epoch 18: val_loss improved from 2.90033 to 2.88178, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.9635 - accuracy: 0.1837 - val_loss: 2.8818 - val_accuracy: 0.2300\n",
      "Epoch 19/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 2.8956 - accuracy: 0.1893\n",
      "Epoch 19: val_loss improved from 2.88178 to 2.86703, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 2.8907 - accuracy: 0.1937 - val_loss: 2.8670 - val_accuracy: 0.2200\n",
      "Epoch 20/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.8147 - accuracy: 0.1886\n",
      "Epoch 20: val_loss did not improve from 2.86703\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.8265 - accuracy: 0.1863 - val_loss: 2.8835 - val_accuracy: 0.2400\n",
      "Epoch 21/500\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 2.8563 - accuracy: 0.1912\n",
      "Epoch 21: val_loss improved from 2.86703 to 2.82019, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.8522 - accuracy: 0.1926 - val_loss: 2.8202 - val_accuracy: 0.2200\n",
      "Epoch 22/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.7842 - accuracy: 0.2080\n",
      "Epoch 22: val_loss did not improve from 2.82019\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.7848 - accuracy: 0.2084 - val_loss: 2.8323 - val_accuracy: 0.2500\n",
      "Epoch 23/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.7838 - accuracy: 0.1918\n",
      "Epoch 23: val_loss improved from 2.82019 to 2.77254, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.7859 - accuracy: 0.1916 - val_loss: 2.7725 - val_accuracy: 0.2700\n",
      "Epoch 24/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.7661 - accuracy: 0.2177\n",
      "Epoch 24: val_loss did not improve from 2.77254\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.7748 - accuracy: 0.2142 - val_loss: 2.7727 - val_accuracy: 0.2600\n",
      "Epoch 25/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.7237 - accuracy: 0.2179\n",
      "Epoch 25: val_loss improved from 2.77254 to 2.76116, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.7237 - accuracy: 0.2179 - val_loss: 2.7612 - val_accuracy: 0.2400\n",
      "Epoch 26/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.7085 - accuracy: 0.2204\n",
      "Epoch 26: val_loss improved from 2.76116 to 2.73040, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.7052 - accuracy: 0.2200 - val_loss: 2.7304 - val_accuracy: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.6484 - accuracy: 0.2458\n",
      "Epoch 27: val_loss did not improve from 2.73040\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.6484 - accuracy: 0.2458 - val_loss: 2.7561 - val_accuracy: 0.2200\n",
      "Epoch 28/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.6274 - accuracy: 0.2453\n",
      "Epoch 28: val_loss improved from 2.73040 to 2.70609, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6274 - accuracy: 0.2453 - val_loss: 2.7061 - val_accuracy: 0.2700\n",
      "Epoch 29/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.6259 - accuracy: 0.2368\n",
      "Epoch 29: val_loss did not improve from 2.70609\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.6273 - accuracy: 0.2384 - val_loss: 2.7169 - val_accuracy: 0.2600\n",
      "Epoch 30/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.6210 - accuracy: 0.2399\n",
      "Epoch 30: val_loss did not improve from 2.70609\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.6194 - accuracy: 0.2405 - val_loss: 2.7115 - val_accuracy: 0.2500\n",
      "Epoch 31/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.5703 - accuracy: 0.2548\n",
      "Epoch 31: val_loss improved from 2.70609 to 2.67998, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5744 - accuracy: 0.2542 - val_loss: 2.6800 - val_accuracy: 0.2800\n",
      "Epoch 32/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.5584 - accuracy: 0.2730\n",
      "Epoch 32: val_loss did not improve from 2.67998\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.5510 - accuracy: 0.2753 - val_loss: 2.6935 - val_accuracy: 0.2100\n",
      "Epoch 33/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.4982 - accuracy: 0.2878\n",
      "Epoch 33: val_loss improved from 2.67998 to 2.66520, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5008 - accuracy: 0.2895 - val_loss: 2.6652 - val_accuracy: 0.2800\n",
      "Epoch 34/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.5345 - accuracy: 0.2807\n",
      "Epoch 34: val_loss did not improve from 2.66520\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.5408 - accuracy: 0.2795 - val_loss: 2.7195 - val_accuracy: 0.2700\n",
      "Epoch 35/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.4939 - accuracy: 0.2725\n",
      "Epoch 35: val_loss improved from 2.66520 to 2.64672, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4967 - accuracy: 0.2732 - val_loss: 2.6467 - val_accuracy: 0.2800\n",
      "Epoch 36/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 2.4347 - accuracy: 0.2930\n",
      "Epoch 36: val_loss improved from 2.64672 to 2.62038, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4421 - accuracy: 0.2937 - val_loss: 2.6204 - val_accuracy: 0.2600\n",
      "Epoch 37/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.4235 - accuracy: 0.3004\n",
      "Epoch 37: val_loss improved from 2.62038 to 2.56320, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4284 - accuracy: 0.2989 - val_loss: 2.5632 - val_accuracy: 0.3000\n",
      "Epoch 38/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.4537 - accuracy: 0.2928\n",
      "Epoch 38: val_loss did not improve from 2.56320\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4493 - accuracy: 0.2953 - val_loss: 2.5728 - val_accuracy: 0.3000\n",
      "Epoch 39/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.4419 - accuracy: 0.3087\n",
      "Epoch 39: val_loss did not improve from 2.56320\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4396 - accuracy: 0.3063 - val_loss: 2.5641 - val_accuracy: 0.3100\n",
      "Epoch 40/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.3467 - accuracy: 0.3270\n",
      "Epoch 40: val_loss improved from 2.56320 to 2.54545, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3502 - accuracy: 0.3232 - val_loss: 2.5455 - val_accuracy: 0.3300\n",
      "Epoch 41/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.3558 - accuracy: 0.3065\n",
      "Epoch 41: val_loss improved from 2.54545 to 2.53670, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3532 - accuracy: 0.3058 - val_loss: 2.5367 - val_accuracy: 0.3000\n",
      "Epoch 42/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.3437 - accuracy: 0.3141\n",
      "Epoch 42: val_loss improved from 2.53670 to 2.53131, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3416 - accuracy: 0.3137 - val_loss: 2.5313 - val_accuracy: 0.3000\n",
      "Epoch 43/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.3153\n",
      "Epoch 43: val_loss did not improve from 2.53131\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3024 - accuracy: 0.3179 - val_loss: 2.5835 - val_accuracy: 0.2700\n",
      "Epoch 44/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.2718 - accuracy: 0.3317\n",
      "Epoch 44: val_loss improved from 2.53131 to 2.51568, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2725 - accuracy: 0.3332 - val_loss: 2.5157 - val_accuracy: 0.3200\n",
      "Epoch 45/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 2.2738 - accuracy: 0.3292\n",
      "Epoch 45: val_loss improved from 2.51568 to 2.49034, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2672 - accuracy: 0.3321 - val_loss: 2.4903 - val_accuracy: 0.3100\n",
      "Epoch 46/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.2455 - accuracy: 0.3421\n",
      "Epoch 46: val_loss did not improve from 2.49034\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2522 - accuracy: 0.3395 - val_loss: 2.5270 - val_accuracy: 0.3700\n",
      "Epoch 47/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.2504 - accuracy: 0.3421\n",
      "Epoch 47: val_loss did not improve from 2.49034\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2491 - accuracy: 0.3411 - val_loss: 2.5123 - val_accuracy: 0.3800\n",
      "Epoch 48/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.2222 - accuracy: 0.3369\n",
      "Epoch 48: val_loss improved from 2.49034 to 2.46064, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2220 - accuracy: 0.3363 - val_loss: 2.4606 - val_accuracy: 0.3700\n",
      "Epoch 49/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 2.2173 - accuracy: 0.3460\n",
      "Epoch 49: val_loss did not improve from 2.46064\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2208 - accuracy: 0.3447 - val_loss: 2.5179 - val_accuracy: 0.3300\n",
      "Epoch 50/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.1832 - accuracy: 0.3498\n",
      "Epoch 50: val_loss did not improve from 2.46064\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1813 - accuracy: 0.3521 - val_loss: 2.5155 - val_accuracy: 0.3000\n",
      "Epoch 51/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 2.1866 - accuracy: 0.3607\n",
      "Epoch 51: val_loss did not improve from 2.46064\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1888 - accuracy: 0.3621 - val_loss: 2.4801 - val_accuracy: 0.3500\n",
      "Epoch 52/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.1610 - accuracy: 0.3712\n",
      "Epoch 52: val_loss improved from 2.46064 to 2.45806, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.1598 - accuracy: 0.3716 - val_loss: 2.4581 - val_accuracy: 0.3600\n",
      "Epoch 53/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.1291 - accuracy: 0.3669\n",
      "Epoch 53: val_loss did not improve from 2.45806\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1255 - accuracy: 0.3689 - val_loss: 2.4644 - val_accuracy: 0.3700\n",
      "Epoch 54/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 2.1297 - accuracy: 0.3733\n",
      "Epoch 54: val_loss improved from 2.45806 to 2.43374, saving model to .\\audio_classification.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 6ms/step - loss: 2.1352 - accuracy: 0.3716 - val_loss: 2.4337 - val_accuracy: 0.3400\n",
      "Epoch 55/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.1058 - accuracy: 0.3726\n",
      "Epoch 55: val_loss improved from 2.43374 to 2.35395, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.1058 - accuracy: 0.3726 - val_loss: 2.3540 - val_accuracy: 0.3500\n",
      "Epoch 56/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.1155 - accuracy: 0.3724\n",
      "Epoch 56: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1136 - accuracy: 0.3732 - val_loss: 2.4209 - val_accuracy: 0.3300\n",
      "Epoch 57/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 2.0944 - accuracy: 0.3801\n",
      "Epoch 57: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0689 - accuracy: 0.3868 - val_loss: 2.3864 - val_accuracy: 0.3700\n",
      "Epoch 58/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.0545 - accuracy: 0.3960\n",
      "Epoch 58: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0582 - accuracy: 0.3968 - val_loss: 2.3751 - val_accuracy: 0.3800\n",
      "Epoch 59/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.0749 - accuracy: 0.3774\n",
      "Epoch 59: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0749 - accuracy: 0.3774 - val_loss: 2.3727 - val_accuracy: 0.3900\n",
      "Epoch 60/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.0610 - accuracy: 0.3900\n",
      "Epoch 60: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0610 - accuracy: 0.3900 - val_loss: 2.3942 - val_accuracy: 0.3800\n",
      "Epoch 61/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 2.0380 - accuracy: 0.4052\n",
      "Epoch 61: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0377 - accuracy: 0.4047 - val_loss: 2.3781 - val_accuracy: 0.3700\n",
      "Epoch 62/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 2.0106 - accuracy: 0.4079\n",
      "Epoch 62: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0115 - accuracy: 0.4063 - val_loss: 2.3962 - val_accuracy: 0.3800\n",
      "Epoch 63/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.9955 - accuracy: 0.4153\n",
      "Epoch 63: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9941 - accuracy: 0.4142 - val_loss: 2.4647 - val_accuracy: 0.3500\n",
      "Epoch 64/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 2.0073 - accuracy: 0.4232\n",
      "Epoch 64: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0073 - accuracy: 0.4232 - val_loss: 2.3778 - val_accuracy: 0.3400\n",
      "Epoch 65/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.9364 - accuracy: 0.4057\n",
      "Epoch 65: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9363 - accuracy: 0.4047 - val_loss: 2.4040 - val_accuracy: 0.3100\n",
      "Epoch 66/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.9550 - accuracy: 0.4235\n",
      "Epoch 66: val_loss did not improve from 2.35395\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9588 - accuracy: 0.4232 - val_loss: 2.4312 - val_accuracy: 0.3300\n",
      "Epoch 67/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.9050 - accuracy: 0.4216\n",
      "Epoch 67: val_loss improved from 2.35395 to 2.34455, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.9064 - accuracy: 0.4216 - val_loss: 2.3445 - val_accuracy: 0.4200\n",
      "Epoch 68/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.9400 - accuracy: 0.4305\n",
      "Epoch 68: val_loss did not improve from 2.34455\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9379 - accuracy: 0.4295 - val_loss: 2.3465 - val_accuracy: 0.3900\n",
      "Epoch 69/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.9329 - accuracy: 0.4105\n",
      "Epoch 69: val_loss improved from 2.34455 to 2.31023, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.9399 - accuracy: 0.4089 - val_loss: 2.3102 - val_accuracy: 0.4200\n",
      "Epoch 70/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.9066 - accuracy: 0.4249\n",
      "Epoch 70: val_loss did not improve from 2.31023\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9036 - accuracy: 0.4253 - val_loss: 2.3529 - val_accuracy: 0.3900\n",
      "Epoch 71/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.9014 - accuracy: 0.4282\n",
      "Epoch 71: val_loss did not improve from 2.31023\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8991 - accuracy: 0.4305 - val_loss: 2.3250 - val_accuracy: 0.4200\n",
      "Epoch 72/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.8897 - accuracy: 0.4327\n",
      "Epoch 72: val_loss did not improve from 2.31023\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8872 - accuracy: 0.4332 - val_loss: 2.3585 - val_accuracy: 0.3800\n",
      "Epoch 73/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.8926 - accuracy: 0.4364\n",
      "Epoch 73: val_loss did not improve from 2.31023\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8929 - accuracy: 0.4358 - val_loss: 2.3391 - val_accuracy: 0.4200\n",
      "Epoch 74/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.8218 - accuracy: 0.4460\n",
      "Epoch 74: val_loss improved from 2.31023 to 2.26726, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.8231 - accuracy: 0.4447 - val_loss: 2.2673 - val_accuracy: 0.3900\n",
      "Epoch 75/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 1.8248 - accuracy: 0.4598\n",
      "Epoch 75: val_loss did not improve from 2.26726\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8271 - accuracy: 0.4616 - val_loss: 2.2969 - val_accuracy: 0.3900\n",
      "Epoch 76/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.8487 - accuracy: 0.4326\n",
      "Epoch 76: val_loss improved from 2.26726 to 2.23883, saving model to .\\audio_classification.hdf5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.8487 - accuracy: 0.4326 - val_loss: 2.2388 - val_accuracy: 0.4100\n",
      "Epoch 77/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.8278 - accuracy: 0.4596\n",
      "Epoch 77: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8257 - accuracy: 0.4611 - val_loss: 2.3002 - val_accuracy: 0.4100\n",
      "Epoch 78/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.7967 - accuracy: 0.4653\n",
      "Epoch 78: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7967 - accuracy: 0.4653 - val_loss: 2.3284 - val_accuracy: 0.4400\n",
      "Epoch 79/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.8247 - accuracy: 0.4495\n",
      "Epoch 79: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8247 - accuracy: 0.4495 - val_loss: 2.2982 - val_accuracy: 0.4000\n",
      "Epoch 80/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.7993 - accuracy: 0.4793\n",
      "Epoch 80: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7993 - accuracy: 0.4784 - val_loss: 2.2479 - val_accuracy: 0.4000\n",
      "Epoch 81/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.7891 - accuracy: 0.4477\n",
      "Epoch 81: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7874 - accuracy: 0.4479 - val_loss: 2.2516 - val_accuracy: 0.4100\n",
      "Epoch 82/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.7689 - accuracy: 0.4663\n",
      "Epoch 82: val_loss did not improve from 2.23883\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7689 - accuracy: 0.4663 - val_loss: 2.3069 - val_accuracy: 0.4200\n",
      "Epoch 83/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.7536 - accuracy: 0.4709\n",
      "Epoch 83: val_loss improved from 2.23883 to 2.22695, saving model to .\\audio_classification.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 6ms/step - loss: 1.7480 - accuracy: 0.4726 - val_loss: 2.2270 - val_accuracy: 0.4400\n",
      "Epoch 84/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.7752 - accuracy: 0.4584\n",
      "Epoch 84: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7752 - accuracy: 0.4584 - val_loss: 2.2615 - val_accuracy: 0.4100\n",
      "Epoch 85/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.7549 - accuracy: 0.4603\n",
      "Epoch 85: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7527 - accuracy: 0.4616 - val_loss: 2.2537 - val_accuracy: 0.4200\n",
      "Epoch 86/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.7416 - accuracy: 0.4778\n",
      "Epoch 86: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7418 - accuracy: 0.4774 - val_loss: 2.2323 - val_accuracy: 0.4100\n",
      "Epoch 87/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.7365 - accuracy: 0.4830\n",
      "Epoch 87: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7369 - accuracy: 0.4821 - val_loss: 2.2999 - val_accuracy: 0.4500\n",
      "Epoch 88/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.6957 - accuracy: 0.4908\n",
      "Epoch 88: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6957 - accuracy: 0.4916 - val_loss: 2.2473 - val_accuracy: 0.4400\n",
      "Epoch 89/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.6848 - accuracy: 0.4767\n",
      "Epoch 89: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6868 - accuracy: 0.4763 - val_loss: 2.3229 - val_accuracy: 0.3900\n",
      "Epoch 90/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.6804 - accuracy: 0.4929\n",
      "Epoch 90: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6777 - accuracy: 0.4947 - val_loss: 2.3506 - val_accuracy: 0.4400\n",
      "Epoch 91/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.6819 - accuracy: 0.4842\n",
      "Epoch 91: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6819 - accuracy: 0.4842 - val_loss: 2.3341 - val_accuracy: 0.4000\n",
      "Epoch 92/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.6677 - accuracy: 0.4871\n",
      "Epoch 92: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6670 - accuracy: 0.4879 - val_loss: 2.3303 - val_accuracy: 0.4300\n",
      "Epoch 93/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.6863 - accuracy: 0.4918\n",
      "Epoch 93: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6847 - accuracy: 0.4905 - val_loss: 2.3353 - val_accuracy: 0.4300\n",
      "Epoch 94/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.6452 - accuracy: 0.4940\n",
      "Epoch 94: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6554 - accuracy: 0.4916 - val_loss: 2.3118 - val_accuracy: 0.4800\n",
      "Epoch 95/500\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 1.6456 - accuracy: 0.5052\n",
      "Epoch 95: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6416 - accuracy: 0.5047 - val_loss: 2.3047 - val_accuracy: 0.4900\n",
      "Epoch 96/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.6444 - accuracy: 0.4907\n",
      "Epoch 96: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6335 - accuracy: 0.4953 - val_loss: 2.3438 - val_accuracy: 0.4100\n",
      "Epoch 97/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.6439 - accuracy: 0.5079\n",
      "Epoch 97: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6419 - accuracy: 0.5084 - val_loss: 2.2937 - val_accuracy: 0.4600\n",
      "Epoch 98/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.6577 - accuracy: 0.4860\n",
      "Epoch 98: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6474 - accuracy: 0.4900 - val_loss: 2.3514 - val_accuracy: 0.4500\n",
      "Epoch 99/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.6179 - accuracy: 0.5033\n",
      "Epoch 99: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6210 - accuracy: 0.5021 - val_loss: 2.3251 - val_accuracy: 0.4800\n",
      "Epoch 100/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.6058 - accuracy: 0.5100\n",
      "Epoch 100: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6100 - accuracy: 0.5089 - val_loss: 2.3827 - val_accuracy: 0.4700\n",
      "Epoch 101/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.6232 - accuracy: 0.5033\n",
      "Epoch 101: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6290 - accuracy: 0.5037 - val_loss: 2.3089 - val_accuracy: 0.4300\n",
      "Epoch 102/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.5903 - accuracy: 0.5075\n",
      "Epoch 102: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5837 - accuracy: 0.5084 - val_loss: 2.4472 - val_accuracy: 0.4700\n",
      "Epoch 103/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5479 - accuracy: 0.5201\n",
      "Epoch 103: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5631 - accuracy: 0.5174 - val_loss: 2.4287 - val_accuracy: 0.4700\n",
      "Epoch 104/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.5464 - accuracy: 0.5318\n",
      "Epoch 104: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5415 - accuracy: 0.5321 - val_loss: 2.3055 - val_accuracy: 0.4500\n",
      "Epoch 105/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.5558 - accuracy: 0.5167\n",
      "Epoch 105: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5532 - accuracy: 0.5153 - val_loss: 2.5217 - val_accuracy: 0.4400\n",
      "Epoch 106/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5708 - accuracy: 0.5156\n",
      "Epoch 106: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5661 - accuracy: 0.5195 - val_loss: 2.4179 - val_accuracy: 0.4300\n",
      "Epoch 107/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5784 - accuracy: 0.5329\n",
      "Epoch 107: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5737 - accuracy: 0.5300 - val_loss: 2.2595 - val_accuracy: 0.4300\n",
      "Epoch 108/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5540 - accuracy: 0.5223\n",
      "Epoch 108: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5511 - accuracy: 0.5247 - val_loss: 2.4008 - val_accuracy: 0.4200\n",
      "Epoch 109/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.5067 - accuracy: 0.5377\n",
      "Epoch 109: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5072 - accuracy: 0.5363 - val_loss: 2.3696 - val_accuracy: 0.4300\n",
      "Epoch 110/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5350 - accuracy: 0.5352\n",
      "Epoch 110: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5339 - accuracy: 0.5368 - val_loss: 2.2800 - val_accuracy: 0.4600\n",
      "Epoch 111/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4885 - accuracy: 0.5444\n",
      "Epoch 111: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4949 - accuracy: 0.5437 - val_loss: 2.4261 - val_accuracy: 0.4500\n",
      "Epoch 112/500\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 1.4909 - accuracy: 0.5358\n",
      "Epoch 112: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4848 - accuracy: 0.5411 - val_loss: 2.3192 - val_accuracy: 0.4500\n",
      "Epoch 113/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5278 - accuracy: 0.5452\n",
      "Epoch 113: val_loss did not improve from 2.22695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5109 - accuracy: 0.5511 - val_loss: 2.3446 - val_accuracy: 0.4800\n",
      "Epoch 114/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4693 - accuracy: 0.5471\n",
      "Epoch 114: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4676 - accuracy: 0.5484 - val_loss: 2.4038 - val_accuracy: 0.4600\n",
      "Epoch 115/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.5160 - accuracy: 0.5251\n",
      "Epoch 115: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5136 - accuracy: 0.5263 - val_loss: 2.4190 - val_accuracy: 0.4300\n",
      "Epoch 116/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.4873 - accuracy: 0.5501\n",
      "Epoch 116: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4807 - accuracy: 0.5532 - val_loss: 2.4050 - val_accuracy: 0.4600\n",
      "Epoch 117/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.5075 - accuracy: 0.5345\n",
      "Epoch 117: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5022 - accuracy: 0.5321 - val_loss: 2.3605 - val_accuracy: 0.4500\n",
      "Epoch 118/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4837 - accuracy: 0.5515\n",
      "Epoch 118: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4899 - accuracy: 0.5484 - val_loss: 2.3816 - val_accuracy: 0.4400\n",
      "Epoch 119/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4460 - accuracy: 0.5692\n",
      "Epoch 119: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4568 - accuracy: 0.5642 - val_loss: 2.3069 - val_accuracy: 0.4900\n",
      "Epoch 120/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4317 - accuracy: 0.5543\n",
      "Epoch 120: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4315 - accuracy: 0.5532 - val_loss: 2.3440 - val_accuracy: 0.4800\n",
      "Epoch 121/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4230 - accuracy: 0.5580\n",
      "Epoch 121: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4304 - accuracy: 0.5563 - val_loss: 2.3625 - val_accuracy: 0.4200\n",
      "Epoch 122/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4741 - accuracy: 0.5564\n",
      "Epoch 122: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4703 - accuracy: 0.5558 - val_loss: 2.3891 - val_accuracy: 0.4500\n",
      "Epoch 123/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.4126 - accuracy: 0.5684\n",
      "Epoch 123: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4128 - accuracy: 0.5695 - val_loss: 2.3925 - val_accuracy: 0.4400\n",
      "Epoch 124/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4412 - accuracy: 0.5592\n",
      "Epoch 124: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4392 - accuracy: 0.5616 - val_loss: 2.3465 - val_accuracy: 0.4500\n",
      "Epoch 125/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.4253 - accuracy: 0.5603\n",
      "Epoch 125: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4369 - accuracy: 0.5558 - val_loss: 2.4215 - val_accuracy: 0.4500\n",
      "Epoch 126/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.4243 - accuracy: 0.5657\n",
      "Epoch 126: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4241 - accuracy: 0.5653 - val_loss: 2.3712 - val_accuracy: 0.4200\n",
      "Epoch 127/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.4392 - accuracy: 0.5592\n",
      "Epoch 127: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4412 - accuracy: 0.5605 - val_loss: 2.3614 - val_accuracy: 0.4700\n",
      "Epoch 128/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3955 - accuracy: 0.5831\n",
      "Epoch 128: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3988 - accuracy: 0.5811 - val_loss: 2.3971 - val_accuracy: 0.4000\n",
      "Epoch 129/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3963 - accuracy: 0.5620\n",
      "Epoch 129: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4020 - accuracy: 0.5595 - val_loss: 2.4420 - val_accuracy: 0.4500\n",
      "Epoch 130/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4474 - accuracy: 0.5541\n",
      "Epoch 130: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4509 - accuracy: 0.5505 - val_loss: 2.4484 - val_accuracy: 0.4500\n",
      "Epoch 131/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4076 - accuracy: 0.5765\n",
      "Epoch 131: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4100 - accuracy: 0.5753 - val_loss: 2.2863 - val_accuracy: 0.4500\n",
      "Epoch 132/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.4031 - accuracy: 0.5684\n",
      "Epoch 132: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4039 - accuracy: 0.5700 - val_loss: 2.3685 - val_accuracy: 0.4500\n",
      "Epoch 133/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.3742 - accuracy: 0.5824\n",
      "Epoch 133: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3752 - accuracy: 0.5832 - val_loss: 2.4076 - val_accuracy: 0.4300\n",
      "Epoch 134/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.4169 - accuracy: 0.5670\n",
      "Epoch 134: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4222 - accuracy: 0.5653 - val_loss: 2.3934 - val_accuracy: 0.4700\n",
      "Epoch 135/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3694 - accuracy: 0.5815\n",
      "Epoch 135: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3742 - accuracy: 0.5758 - val_loss: 2.3706 - val_accuracy: 0.4400\n",
      "Epoch 136/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3531 - accuracy: 0.5848\n",
      "Epoch 136: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3551 - accuracy: 0.5847 - val_loss: 2.3889 - val_accuracy: 0.4700\n",
      "Epoch 137/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3945 - accuracy: 0.5746\n",
      "Epoch 137: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3931 - accuracy: 0.5737 - val_loss: 2.2937 - val_accuracy: 0.4700\n",
      "Epoch 138/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.3685 - accuracy: 0.5717\n",
      "Epoch 138: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3716 - accuracy: 0.5711 - val_loss: 2.2910 - val_accuracy: 0.4800\n",
      "Epoch 139/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.3822 - accuracy: 0.5647\n",
      "Epoch 139: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3877 - accuracy: 0.5632 - val_loss: 2.2618 - val_accuracy: 0.5100\n",
      "Epoch 140/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3204 - accuracy: 0.6004\n",
      "Epoch 140: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3292 - accuracy: 0.5958 - val_loss: 2.4548 - val_accuracy: 0.4900\n",
      "Epoch 141/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.3538 - accuracy: 0.5862\n",
      "Epoch 141: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3488 - accuracy: 0.5879 - val_loss: 2.3075 - val_accuracy: 0.4600\n",
      "Epoch 142/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3415 - accuracy: 0.5844\n",
      "Epoch 142: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3488 - accuracy: 0.5842 - val_loss: 2.4023 - val_accuracy: 0.4900\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3425 - accuracy: 0.5877\n",
      "Epoch 143: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3478 - accuracy: 0.5889 - val_loss: 2.3454 - val_accuracy: 0.5000\n",
      "Epoch 144/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3496 - accuracy: 0.5781\n",
      "Epoch 144: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3477 - accuracy: 0.5800 - val_loss: 2.3825 - val_accuracy: 0.4600\n",
      "Epoch 145/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3073 - accuracy: 0.5943\n",
      "Epoch 145: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3035 - accuracy: 0.5932 - val_loss: 2.4277 - val_accuracy: 0.4900\n",
      "Epoch 146/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.3685 - accuracy: 0.5760\n",
      "Epoch 146: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3641 - accuracy: 0.5784 - val_loss: 2.4567 - val_accuracy: 0.5000\n",
      "Epoch 147/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3323 - accuracy: 0.5915\n",
      "Epoch 147: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3356 - accuracy: 0.5900 - val_loss: 2.4745 - val_accuracy: 0.4700\n",
      "Epoch 148/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.3148 - accuracy: 0.5910\n",
      "Epoch 148: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3163 - accuracy: 0.5905 - val_loss: 2.4452 - val_accuracy: 0.4800\n",
      "Epoch 149/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.2630 - accuracy: 0.6055\n",
      "Epoch 149: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2759 - accuracy: 0.5989 - val_loss: 2.4295 - val_accuracy: 0.5100\n",
      "Epoch 150/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2799 - accuracy: 0.6124\n",
      "Epoch 150: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2859 - accuracy: 0.6100 - val_loss: 2.3989 - val_accuracy: 0.4800\n",
      "Epoch 151/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.2826 - accuracy: 0.6043\n",
      "Epoch 151: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2793 - accuracy: 0.6058 - val_loss: 2.3829 - val_accuracy: 0.4900\n",
      "Epoch 152/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2944 - accuracy: 0.6094\n",
      "Epoch 152: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2981 - accuracy: 0.6105 - val_loss: 2.4371 - val_accuracy: 0.4900\n",
      "Epoch 153/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.3279 - accuracy: 0.5890\n",
      "Epoch 153: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3255 - accuracy: 0.5905 - val_loss: 2.3833 - val_accuracy: 0.4800\n",
      "Epoch 154/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.2987 - accuracy: 0.6043\n",
      "Epoch 154: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2965 - accuracy: 0.6053 - val_loss: 2.4226 - val_accuracy: 0.5100\n",
      "Epoch 155/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.2722 - accuracy: 0.6298\n",
      "Epoch 155: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2717 - accuracy: 0.6295 - val_loss: 2.4590 - val_accuracy: 0.4800\n",
      "Epoch 156/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2719 - accuracy: 0.5959\n",
      "Epoch 156: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2659 - accuracy: 0.5989 - val_loss: 2.5711 - val_accuracy: 0.5000\n",
      "Epoch 157/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2867 - accuracy: 0.5981\n",
      "Epoch 157: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2830 - accuracy: 0.5979 - val_loss: 2.4890 - val_accuracy: 0.5300\n",
      "Epoch 158/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2810 - accuracy: 0.5997\n",
      "Epoch 158: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2883 - accuracy: 0.5979 - val_loss: 2.4605 - val_accuracy: 0.4800\n",
      "Epoch 159/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2544 - accuracy: 0.6234\n",
      "Epoch 159: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2584 - accuracy: 0.6226 - val_loss: 2.4558 - val_accuracy: 0.4700\n",
      "Epoch 160/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.2629 - accuracy: 0.6077\n",
      "Epoch 160: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2625 - accuracy: 0.6084 - val_loss: 2.5044 - val_accuracy: 0.4600\n",
      "Epoch 161/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1909 - accuracy: 0.6420\n",
      "Epoch 161: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2021 - accuracy: 0.6416 - val_loss: 2.5250 - val_accuracy: 0.5100\n",
      "Epoch 162/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.2409 - accuracy: 0.6161\n",
      "Epoch 162: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2365 - accuracy: 0.6142 - val_loss: 2.4093 - val_accuracy: 0.5300\n",
      "Epoch 163/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1848 - accuracy: 0.6184\n",
      "Epoch 163: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1839 - accuracy: 0.6221 - val_loss: 2.5547 - val_accuracy: 0.4800\n",
      "Epoch 164/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.2852 - accuracy: 0.6096\n",
      "Epoch 164: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2845 - accuracy: 0.6095 - val_loss: 2.5109 - val_accuracy: 0.5000\n",
      "Epoch 165/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2574 - accuracy: 0.6029\n",
      "Epoch 165: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2544 - accuracy: 0.6047 - val_loss: 2.4945 - val_accuracy: 0.4800\n",
      "Epoch 166/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2163 - accuracy: 0.6228\n",
      "Epoch 166: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2215 - accuracy: 0.6221 - val_loss: 2.5810 - val_accuracy: 0.4600\n",
      "Epoch 167/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2120 - accuracy: 0.6304\n",
      "Epoch 167: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2116 - accuracy: 0.6305 - val_loss: 2.4809 - val_accuracy: 0.5300\n",
      "Epoch 168/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.2411 - accuracy: 0.6163\n",
      "Epoch 168: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2411 - accuracy: 0.6163 - val_loss: 2.5423 - val_accuracy: 0.5100\n",
      "Epoch 169/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.2861 - accuracy: 0.6010\n",
      "Epoch 169: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2844 - accuracy: 0.6026 - val_loss: 2.4781 - val_accuracy: 0.5000\n",
      "Epoch 170/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.2389 - accuracy: 0.6229\n",
      "Epoch 170: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2399 - accuracy: 0.6226 - val_loss: 2.4743 - val_accuracy: 0.4800\n",
      "Epoch 171/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2157 - accuracy: 0.6184\n",
      "Epoch 171: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2175 - accuracy: 0.6174 - val_loss: 2.5229 - val_accuracy: 0.5100\n",
      "Epoch 172/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1607 - accuracy: 0.6360\n",
      "Epoch 172: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1529 - accuracy: 0.6332 - val_loss: 2.5381 - val_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2023 - accuracy: 0.6349\n",
      "Epoch 173: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1965 - accuracy: 0.6358 - val_loss: 2.6107 - val_accuracy: 0.4900\n",
      "Epoch 174/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1933 - accuracy: 0.6294\n",
      "Epoch 174: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1936 - accuracy: 0.6300 - val_loss: 2.6116 - val_accuracy: 0.4900\n",
      "Epoch 175/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2210 - accuracy: 0.6175\n",
      "Epoch 175: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2237 - accuracy: 0.6158 - val_loss: 2.5892 - val_accuracy: 0.4900\n",
      "Epoch 176/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1876 - accuracy: 0.6289\n",
      "Epoch 176: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1876 - accuracy: 0.6289 - val_loss: 2.5385 - val_accuracy: 0.4900\n",
      "Epoch 177/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1942 - accuracy: 0.6245\n",
      "Epoch 177: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1947 - accuracy: 0.6242 - val_loss: 2.4467 - val_accuracy: 0.4700\n",
      "Epoch 178/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.1716 - accuracy: 0.6523\n",
      "Epoch 178: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1635 - accuracy: 0.6526 - val_loss: 2.4992 - val_accuracy: 0.5100\n",
      "Epoch 179/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1806 - accuracy: 0.6321\n",
      "Epoch 179: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1709 - accuracy: 0.6363 - val_loss: 2.4905 - val_accuracy: 0.4400\n",
      "Epoch 180/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1397 - accuracy: 0.6420\n",
      "Epoch 180: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1429 - accuracy: 0.6395 - val_loss: 2.5475 - val_accuracy: 0.4400\n",
      "Epoch 181/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1480 - accuracy: 0.6460\n",
      "Epoch 181: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1495 - accuracy: 0.6442 - val_loss: 2.4791 - val_accuracy: 0.5000\n",
      "Epoch 182/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1110 - accuracy: 0.6541\n",
      "Epoch 182: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1142 - accuracy: 0.6537 - val_loss: 2.4625 - val_accuracy: 0.5000\n",
      "Epoch 183/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1949 - accuracy: 0.6245\n",
      "Epoch 183: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2029 - accuracy: 0.6232 - val_loss: 2.5325 - val_accuracy: 0.4900\n",
      "Epoch 184/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.2091 - accuracy: 0.6491\n",
      "Epoch 184: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2156 - accuracy: 0.6458 - val_loss: 2.4116 - val_accuracy: 0.4900\n",
      "Epoch 185/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1959 - accuracy: 0.6321\n",
      "Epoch 185: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1964 - accuracy: 0.6316 - val_loss: 2.4295 - val_accuracy: 0.5000\n",
      "Epoch 186/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1646 - accuracy: 0.6315\n",
      "Epoch 186: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1575 - accuracy: 0.6353 - val_loss: 2.4744 - val_accuracy: 0.5000\n",
      "Epoch 187/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2135 - accuracy: 0.6331\n",
      "Epoch 187: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2121 - accuracy: 0.6353 - val_loss: 2.4455 - val_accuracy: 0.5100\n",
      "Epoch 188/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1236 - accuracy: 0.6589\n",
      "Epoch 188: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1292 - accuracy: 0.6579 - val_loss: 2.5265 - val_accuracy: 0.4800\n",
      "Epoch 189/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1538 - accuracy: 0.6557\n",
      "Epoch 189: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1568 - accuracy: 0.6532 - val_loss: 2.6190 - val_accuracy: 0.4500\n",
      "Epoch 190/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1753 - accuracy: 0.6411\n",
      "Epoch 190: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1753 - accuracy: 0.6411 - val_loss: 2.4862 - val_accuracy: 0.5300\n",
      "Epoch 191/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1418 - accuracy: 0.6480\n",
      "Epoch 191: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1463 - accuracy: 0.6468 - val_loss: 2.4446 - val_accuracy: 0.5100\n",
      "Epoch 192/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.1726 - accuracy: 0.6445\n",
      "Epoch 192: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1740 - accuracy: 0.6437 - val_loss: 2.5214 - val_accuracy: 0.4900\n",
      "Epoch 193/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1561 - accuracy: 0.6494\n",
      "Epoch 193: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1571 - accuracy: 0.6489 - val_loss: 2.4332 - val_accuracy: 0.4900\n",
      "Epoch 194/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.1396 - accuracy: 0.6496\n",
      "Epoch 194: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1327 - accuracy: 0.6511 - val_loss: 2.3159 - val_accuracy: 0.5000\n",
      "Epoch 195/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0994 - accuracy: 0.6502\n",
      "Epoch 195: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0958 - accuracy: 0.6500 - val_loss: 2.5002 - val_accuracy: 0.5100\n",
      "Epoch 196/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1340 - accuracy: 0.6455\n",
      "Epoch 196: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1298 - accuracy: 0.6468 - val_loss: 2.5137 - val_accuracy: 0.4900\n",
      "Epoch 197/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1151 - accuracy: 0.6476\n",
      "Epoch 197: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1143 - accuracy: 0.6474 - val_loss: 2.5527 - val_accuracy: 0.5300\n",
      "Epoch 198/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1322 - accuracy: 0.6601\n",
      "Epoch 198: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1246 - accuracy: 0.6605 - val_loss: 2.5471 - val_accuracy: 0.4800\n",
      "Epoch 199/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.2024 - accuracy: 0.6239\n",
      "Epoch 199: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1970 - accuracy: 0.6247 - val_loss: 2.5412 - val_accuracy: 0.4900\n",
      "Epoch 200/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1394 - accuracy: 0.6525\n",
      "Epoch 200: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1386 - accuracy: 0.6516 - val_loss: 2.5248 - val_accuracy: 0.4700\n",
      "Epoch 201/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1734 - accuracy: 0.6472\n",
      "Epoch 201: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1746 - accuracy: 0.6463 - val_loss: 2.4958 - val_accuracy: 0.5000\n",
      "Epoch 202/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0943 - accuracy: 0.6711\n",
      "Epoch 202: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0943 - accuracy: 0.6711 - val_loss: 2.4543 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0932 - accuracy: 0.6463\n",
      "Epoch 203: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0932 - accuracy: 0.6463 - val_loss: 2.5960 - val_accuracy: 0.4900\n",
      "Epoch 204/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0669 - accuracy: 0.6816\n",
      "Epoch 204: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0647 - accuracy: 0.6811 - val_loss: 2.4632 - val_accuracy: 0.5100\n",
      "Epoch 205/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1068 - accuracy: 0.6594\n",
      "Epoch 205: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1038 - accuracy: 0.6600 - val_loss: 2.4915 - val_accuracy: 0.4800\n",
      "Epoch 206/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1215 - accuracy: 0.6453\n",
      "Epoch 206: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1215 - accuracy: 0.6453 - val_loss: 2.4710 - val_accuracy: 0.4700\n",
      "Epoch 207/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1137 - accuracy: 0.6645\n",
      "Epoch 207: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1201 - accuracy: 0.6616 - val_loss: 2.5119 - val_accuracy: 0.4600\n",
      "Epoch 208/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1149 - accuracy: 0.6590\n",
      "Epoch 208: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1126 - accuracy: 0.6600 - val_loss: 2.5936 - val_accuracy: 0.4900\n",
      "Epoch 209/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1249 - accuracy: 0.6463\n",
      "Epoch 209: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1249 - accuracy: 0.6463 - val_loss: 2.5954 - val_accuracy: 0.4600\n",
      "Epoch 210/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.1045 - accuracy: 0.6455\n",
      "Epoch 210: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1034 - accuracy: 0.6474 - val_loss: 2.5425 - val_accuracy: 0.4800\n",
      "Epoch 211/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1154 - accuracy: 0.6679\n",
      "Epoch 211: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1154 - accuracy: 0.6679 - val_loss: 2.4947 - val_accuracy: 0.5100\n",
      "Epoch 212/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0458 - accuracy: 0.6737\n",
      "Epoch 212: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0458 - accuracy: 0.6737 - val_loss: 2.5390 - val_accuracy: 0.4900\n",
      "Epoch 213/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0855 - accuracy: 0.6697\n",
      "Epoch 213: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0872 - accuracy: 0.6705 - val_loss: 2.5304 - val_accuracy: 0.4800\n",
      "Epoch 214/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.1255 - accuracy: 0.6653\n",
      "Epoch 214: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1238 - accuracy: 0.6653 - val_loss: 2.5126 - val_accuracy: 0.5000\n",
      "Epoch 215/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0934 - accuracy: 0.6647\n",
      "Epoch 215: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0908 - accuracy: 0.6653 - val_loss: 2.5530 - val_accuracy: 0.5400\n",
      "Epoch 216/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 1.1033 - accuracy: 0.6665\n",
      "Epoch 216: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0989 - accuracy: 0.6632 - val_loss: 2.5714 - val_accuracy: 0.4700\n",
      "Epoch 217/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0473 - accuracy: 0.6805\n",
      "Epoch 217: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0496 - accuracy: 0.6784 - val_loss: 2.4727 - val_accuracy: 0.5200\n",
      "Epoch 218/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0781 - accuracy: 0.6692\n",
      "Epoch 218: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0692 - accuracy: 0.6711 - val_loss: 2.5433 - val_accuracy: 0.5300\n",
      "Epoch 219/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0282 - accuracy: 0.6801\n",
      "Epoch 219: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0271 - accuracy: 0.6805 - val_loss: 2.7001 - val_accuracy: 0.5100\n",
      "Epoch 220/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1150 - accuracy: 0.6524\n",
      "Epoch 220: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1253 - accuracy: 0.6511 - val_loss: 2.6108 - val_accuracy: 0.5100\n",
      "Epoch 221/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0766 - accuracy: 0.6649\n",
      "Epoch 221: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0820 - accuracy: 0.6621 - val_loss: 2.6221 - val_accuracy: 0.5300\n",
      "Epoch 222/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0741 - accuracy: 0.6730\n",
      "Epoch 222: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0735 - accuracy: 0.6737 - val_loss: 2.5591 - val_accuracy: 0.5300\n",
      "Epoch 223/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0795 - accuracy: 0.6738\n",
      "Epoch 223: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0819 - accuracy: 0.6705 - val_loss: 2.6004 - val_accuracy: 0.4900\n",
      "Epoch 224/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0660 - accuracy: 0.6811\n",
      "Epoch 224: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0632 - accuracy: 0.6821 - val_loss: 2.5238 - val_accuracy: 0.5400\n",
      "Epoch 225/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0871 - accuracy: 0.6706\n",
      "Epoch 225: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0866 - accuracy: 0.6711 - val_loss: 2.5476 - val_accuracy: 0.5000\n",
      "Epoch 226/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0451 - accuracy: 0.6811\n",
      "Epoch 226: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0464 - accuracy: 0.6811 - val_loss: 2.5662 - val_accuracy: 0.5200\n",
      "Epoch 227/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0321 - accuracy: 0.6859\n",
      "Epoch 227: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0328 - accuracy: 0.6842 - val_loss: 2.4907 - val_accuracy: 0.4700\n",
      "Epoch 228/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0229 - accuracy: 0.6804\n",
      "Epoch 228: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0200 - accuracy: 0.6816 - val_loss: 2.6869 - val_accuracy: 0.5000\n",
      "Epoch 229/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0559 - accuracy: 0.6785\n",
      "Epoch 229: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0596 - accuracy: 0.6779 - val_loss: 2.6585 - val_accuracy: 0.5000\n",
      "Epoch 230/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.0910 - accuracy: 0.6529\n",
      "Epoch 230: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0996 - accuracy: 0.6537 - val_loss: 2.6451 - val_accuracy: 0.4700\n",
      "Epoch 231/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0499 - accuracy: 0.6634\n",
      "Epoch 231: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0581 - accuracy: 0.6584 - val_loss: 2.6251 - val_accuracy: 0.4800\n",
      "Epoch 232/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0241 - accuracy: 0.6764\n",
      "Epoch 232: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0258 - accuracy: 0.6763 - val_loss: 2.5257 - val_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0324 - accuracy: 0.6848\n",
      "Epoch 233: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0424 - accuracy: 0.6821 - val_loss: 2.6049 - val_accuracy: 0.4900\n",
      "Epoch 234/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0452 - accuracy: 0.6765\n",
      "Epoch 234: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0333 - accuracy: 0.6816 - val_loss: 2.6460 - val_accuracy: 0.5200\n",
      "Epoch 235/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.0406 - accuracy: 0.6881\n",
      "Epoch 235: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0477 - accuracy: 0.6874 - val_loss: 2.6422 - val_accuracy: 0.4800\n",
      "Epoch 236/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0872 - accuracy: 0.6628\n",
      "Epoch 236: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0812 - accuracy: 0.6637 - val_loss: 2.8003 - val_accuracy: 0.5000\n",
      "Epoch 237/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0225 - accuracy: 0.6754\n",
      "Epoch 237: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0266 - accuracy: 0.6742 - val_loss: 2.6966 - val_accuracy: 0.5300\n",
      "Epoch 238/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0162 - accuracy: 0.6858\n",
      "Epoch 238: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0162 - accuracy: 0.6858 - val_loss: 2.8321 - val_accuracy: 0.5200\n",
      "Epoch 239/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0464 - accuracy: 0.6970\n",
      "Epoch 239: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0421 - accuracy: 0.6984 - val_loss: 2.7588 - val_accuracy: 0.5300\n",
      "Epoch 240/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.6751\n",
      "Epoch 240: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0091 - accuracy: 0.6758 - val_loss: 2.7116 - val_accuracy: 0.5200\n",
      "Epoch 241/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0113 - accuracy: 0.6907\n",
      "Epoch 241: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0099 - accuracy: 0.6895 - val_loss: 2.6913 - val_accuracy: 0.5300\n",
      "Epoch 242/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0353 - accuracy: 0.6880\n",
      "Epoch 242: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0324 - accuracy: 0.6895 - val_loss: 2.6091 - val_accuracy: 0.4600\n",
      "Epoch 243/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 1.0172 - accuracy: 0.6856\n",
      "Epoch 243: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0193 - accuracy: 0.6853 - val_loss: 2.5730 - val_accuracy: 0.5300\n",
      "Epoch 244/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0215 - accuracy: 0.6960\n",
      "Epoch 244: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0225 - accuracy: 0.6953 - val_loss: 2.8132 - val_accuracy: 0.4600\n",
      "Epoch 245/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0323 - accuracy: 0.6905\n",
      "Epoch 245: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0323 - accuracy: 0.6905 - val_loss: 2.6385 - val_accuracy: 0.4900\n",
      "Epoch 246/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9714 - accuracy: 0.7013\n",
      "Epoch 246: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9684 - accuracy: 0.7021 - val_loss: 2.7466 - val_accuracy: 0.5200\n",
      "Epoch 247/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9945 - accuracy: 0.6913\n",
      "Epoch 247: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0009 - accuracy: 0.6932 - val_loss: 2.7144 - val_accuracy: 0.4900\n",
      "Epoch 248/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0001 - accuracy: 0.6837\n",
      "Epoch 248: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0001 - accuracy: 0.6837 - val_loss: 2.7841 - val_accuracy: 0.5100\n",
      "Epoch 249/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9676 - accuracy: 0.7050\n",
      "Epoch 249: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9700 - accuracy: 0.7042 - val_loss: 2.7222 - val_accuracy: 0.4900\n",
      "Epoch 250/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9977 - accuracy: 0.6859\n",
      "Epoch 250: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9983 - accuracy: 0.6863 - val_loss: 2.8593 - val_accuracy: 0.4900\n",
      "Epoch 251/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0194 - accuracy: 0.6870\n",
      "Epoch 251: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0183 - accuracy: 0.6863 - val_loss: 2.6527 - val_accuracy: 0.5100\n",
      "Epoch 252/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0002 - accuracy: 0.7001\n",
      "Epoch 252: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0061 - accuracy: 0.6979 - val_loss: 2.6362 - val_accuracy: 0.4700\n",
      "Epoch 253/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9559 - accuracy: 0.7100\n",
      "Epoch 253: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9621 - accuracy: 0.7089 - val_loss: 2.6686 - val_accuracy: 0.4800\n",
      "Epoch 254/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0048 - accuracy: 0.6912\n",
      "Epoch 254: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0065 - accuracy: 0.6911 - val_loss: 2.6119 - val_accuracy: 0.5200\n",
      "Epoch 255/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0259 - accuracy: 0.6760\n",
      "Epoch 255: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0186 - accuracy: 0.6768 - val_loss: 2.6997 - val_accuracy: 0.4900\n",
      "Epoch 256/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.0058 - accuracy: 0.6935\n",
      "Epoch 256: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0213 - accuracy: 0.6921 - val_loss: 2.6662 - val_accuracy: 0.5000\n",
      "Epoch 257/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0344 - accuracy: 0.6716\n",
      "Epoch 257: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0344 - accuracy: 0.6716 - val_loss: 2.6839 - val_accuracy: 0.5100\n",
      "Epoch 258/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9539 - accuracy: 0.7023\n",
      "Epoch 258: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9555 - accuracy: 0.7016 - val_loss: 2.7669 - val_accuracy: 0.4900\n",
      "Epoch 259/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9308 - accuracy: 0.7037\n",
      "Epoch 259: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9284 - accuracy: 0.7063 - val_loss: 2.5899 - val_accuracy: 0.4600\n",
      "Epoch 260/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9556 - accuracy: 0.7161\n",
      "Epoch 260: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9579 - accuracy: 0.7153 - val_loss: 2.6708 - val_accuracy: 0.5100\n",
      "Epoch 261/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9968 - accuracy: 0.6946\n",
      "Epoch 261: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9942 - accuracy: 0.6942 - val_loss: 2.7785 - val_accuracy: 0.5100\n",
      "Epoch 262/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9936 - accuracy: 0.7023\n",
      "Epoch 262: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9948 - accuracy: 0.7016 - val_loss: 2.7865 - val_accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9949 - accuracy: 0.6880\n",
      "Epoch 263: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9868 - accuracy: 0.6916 - val_loss: 2.7203 - val_accuracy: 0.5100\n",
      "Epoch 264/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9415 - accuracy: 0.7166\n",
      "Epoch 264: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9428 - accuracy: 0.7147 - val_loss: 2.7521 - val_accuracy: 0.4800\n",
      "Epoch 265/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9701 - accuracy: 0.6988\n",
      "Epoch 265: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9645 - accuracy: 0.7016 - val_loss: 2.7346 - val_accuracy: 0.5000\n",
      "Epoch 266/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0095 - accuracy: 0.6874\n",
      "Epoch 266: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0095 - accuracy: 0.6874 - val_loss: 2.8856 - val_accuracy: 0.5100\n",
      "Epoch 267/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9712 - accuracy: 0.6926\n",
      "Epoch 267: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9712 - accuracy: 0.6926 - val_loss: 2.8838 - val_accuracy: 0.4700\n",
      "Epoch 268/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9779 - accuracy: 0.6843\n",
      "Epoch 268: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9777 - accuracy: 0.6847 - val_loss: 2.6549 - val_accuracy: 0.5000\n",
      "Epoch 269/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9453 - accuracy: 0.7168\n",
      "Epoch 269: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9378 - accuracy: 0.7137 - val_loss: 2.7941 - val_accuracy: 0.4700\n",
      "Epoch 270/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9397 - accuracy: 0.7226\n",
      "Epoch 270: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9590 - accuracy: 0.7163 - val_loss: 2.7750 - val_accuracy: 0.4800\n",
      "Epoch 271/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 1.0304 - accuracy: 0.7064\n",
      "Epoch 271: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0249 - accuracy: 0.7068 - val_loss: 2.8157 - val_accuracy: 0.5600\n",
      "Epoch 272/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9132 - accuracy: 0.7142\n",
      "Epoch 272: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9132 - accuracy: 0.7142 - val_loss: 2.8153 - val_accuracy: 0.5000\n",
      "Epoch 273/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0317 - accuracy: 0.7011\n",
      "Epoch 273: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0317 - accuracy: 0.7011 - val_loss: 2.7685 - val_accuracy: 0.5500\n",
      "Epoch 274/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9513 - accuracy: 0.7013\n",
      "Epoch 274: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9536 - accuracy: 0.7005 - val_loss: 2.8655 - val_accuracy: 0.5100\n",
      "Epoch 275/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9513 - accuracy: 0.7111\n",
      "Epoch 275: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9640 - accuracy: 0.7089 - val_loss: 2.8704 - val_accuracy: 0.5100\n",
      "Epoch 276/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9796 - accuracy: 0.7105\n",
      "Epoch 276: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9548 - accuracy: 0.7184 - val_loss: 2.9113 - val_accuracy: 0.5000\n",
      "Epoch 277/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9619 - accuracy: 0.7143\n",
      "Epoch 277: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9595 - accuracy: 0.7132 - val_loss: 2.8507 - val_accuracy: 0.5200\n",
      "Epoch 278/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9673 - accuracy: 0.7026\n",
      "Epoch 278: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9673 - accuracy: 0.7026 - val_loss: 2.8017 - val_accuracy: 0.5200\n",
      "Epoch 279/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9614 - accuracy: 0.7098\n",
      "Epoch 279: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9759 - accuracy: 0.7053 - val_loss: 2.8528 - val_accuracy: 0.5300\n",
      "Epoch 280/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9261 - accuracy: 0.7156\n",
      "Epoch 280: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9287 - accuracy: 0.7137 - val_loss: 2.8913 - val_accuracy: 0.4800\n",
      "Epoch 281/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9205 - accuracy: 0.7156\n",
      "Epoch 281: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9203 - accuracy: 0.7153 - val_loss: 2.7265 - val_accuracy: 0.5100\n",
      "Epoch 282/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9432 - accuracy: 0.7089\n",
      "Epoch 282: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9432 - accuracy: 0.7089 - val_loss: 2.8692 - val_accuracy: 0.5300\n",
      "Epoch 283/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.7263\n",
      "Epoch 283: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9122 - accuracy: 0.7263 - val_loss: 2.8645 - val_accuracy: 0.5200\n",
      "Epoch 284/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8843 - accuracy: 0.7095\n",
      "Epoch 284: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8843 - accuracy: 0.7095 - val_loss: 2.9624 - val_accuracy: 0.4900\n",
      "Epoch 285/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0113 - accuracy: 0.6989\n",
      "Epoch 285: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0113 - accuracy: 0.6989 - val_loss: 3.0811 - val_accuracy: 0.5200\n",
      "Epoch 286/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9173 - accuracy: 0.7193\n",
      "Epoch 286: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9149 - accuracy: 0.7200 - val_loss: 2.8481 - val_accuracy: 0.5000\n",
      "Epoch 287/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.9338 - accuracy: 0.7181\n",
      "Epoch 287: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9322 - accuracy: 0.7174 - val_loss: 2.8614 - val_accuracy: 0.5200\n",
      "Epoch 288/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9902 - accuracy: 0.7042\n",
      "Epoch 288: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9902 - accuracy: 0.7042 - val_loss: 2.7326 - val_accuracy: 0.5600\n",
      "Epoch 289/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8802 - accuracy: 0.7374\n",
      "Epoch 289: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8802 - accuracy: 0.7374 - val_loss: 2.8882 - val_accuracy: 0.5300\n",
      "Epoch 290/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9270 - accuracy: 0.7211\n",
      "Epoch 290: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9270 - accuracy: 0.7211 - val_loss: 2.9664 - val_accuracy: 0.5300\n",
      "Epoch 291/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.7095\n",
      "Epoch 291: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9312 - accuracy: 0.7095 - val_loss: 2.9404 - val_accuracy: 0.5500\n",
      "Epoch 292/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9596 - accuracy: 0.6963\n",
      "Epoch 292: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9596 - accuracy: 0.6963 - val_loss: 2.9621 - val_accuracy: 0.5700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9755 - accuracy: 0.7021\n",
      "Epoch 293: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9755 - accuracy: 0.7021 - val_loss: 3.0203 - val_accuracy: 0.4900\n",
      "Epoch 294/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8856 - accuracy: 0.7326\n",
      "Epoch 294: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8856 - accuracy: 0.7326 - val_loss: 2.9509 - val_accuracy: 0.4900\n",
      "Epoch 295/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9441 - accuracy: 0.7100\n",
      "Epoch 295: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9441 - accuracy: 0.7100 - val_loss: 3.0533 - val_accuracy: 0.5100\n",
      "Epoch 296/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9170 - accuracy: 0.7256\n",
      "Epoch 296: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9164 - accuracy: 0.7258 - val_loss: 3.0061 - val_accuracy: 0.5000\n",
      "Epoch 297/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9332 - accuracy: 0.7216\n",
      "Epoch 297: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9332 - accuracy: 0.7216 - val_loss: 2.9108 - val_accuracy: 0.5500\n",
      "Epoch 298/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9299 - accuracy: 0.7132\n",
      "Epoch 298: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9299 - accuracy: 0.7132 - val_loss: 2.9181 - val_accuracy: 0.5600\n",
      "Epoch 299/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.9278 - accuracy: 0.7217\n",
      "Epoch 299: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9197 - accuracy: 0.7279 - val_loss: 3.0424 - val_accuracy: 0.4900\n",
      "Epoch 300/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8863 - accuracy: 0.7334\n",
      "Epoch 300: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8979 - accuracy: 0.7268 - val_loss: 3.0015 - val_accuracy: 0.5300\n",
      "Epoch 301/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8909 - accuracy: 0.7268\n",
      "Epoch 301: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8924 - accuracy: 0.7247 - val_loss: 3.0834 - val_accuracy: 0.5100\n",
      "Epoch 302/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8837 - accuracy: 0.7258\n",
      "Epoch 302: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8809 - accuracy: 0.7274 - val_loss: 3.1671 - val_accuracy: 0.4800\n",
      "Epoch 303/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8292 - accuracy: 0.7403\n",
      "Epoch 303: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8281 - accuracy: 0.7400 - val_loss: 3.0212 - val_accuracy: 0.4900\n",
      "Epoch 304/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8276 - accuracy: 0.7522\n",
      "Epoch 304: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8356 - accuracy: 0.7500 - val_loss: 2.7892 - val_accuracy: 0.5400\n",
      "Epoch 305/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9085 - accuracy: 0.7135\n",
      "Epoch 305: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9089 - accuracy: 0.7132 - val_loss: 2.9446 - val_accuracy: 0.5600\n",
      "Epoch 306/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9444 - accuracy: 0.7230\n",
      "Epoch 306: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9472 - accuracy: 0.7216 - val_loss: 2.9775 - val_accuracy: 0.4900\n",
      "Epoch 307/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9317 - accuracy: 0.7058\n",
      "Epoch 307: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9357 - accuracy: 0.7063 - val_loss: 2.8657 - val_accuracy: 0.5100\n",
      "Epoch 308/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9037 - accuracy: 0.7112\n",
      "Epoch 308: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9022 - accuracy: 0.7111 - val_loss: 2.9601 - val_accuracy: 0.5100\n",
      "Epoch 309/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8840 - accuracy: 0.7331\n",
      "Epoch 309: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8828 - accuracy: 0.7337 - val_loss: 2.9556 - val_accuracy: 0.4800\n",
      "Epoch 310/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8964 - accuracy: 0.7252\n",
      "Epoch 310: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8964 - accuracy: 0.7232 - val_loss: 2.8111 - val_accuracy: 0.5500\n",
      "Epoch 311/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8747 - accuracy: 0.7322\n",
      "Epoch 311: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8762 - accuracy: 0.7316 - val_loss: 2.9277 - val_accuracy: 0.4900\n",
      "Epoch 312/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8900 - accuracy: 0.7214\n",
      "Epoch 312: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8894 - accuracy: 0.7211 - val_loss: 2.9881 - val_accuracy: 0.4900\n",
      "Epoch 313/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8713 - accuracy: 0.7309\n",
      "Epoch 313: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8706 - accuracy: 0.7316 - val_loss: 3.0524 - val_accuracy: 0.4900\n",
      "Epoch 314/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9035 - accuracy: 0.7274\n",
      "Epoch 314: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9022 - accuracy: 0.7300 - val_loss: 2.8676 - val_accuracy: 0.4900\n",
      "Epoch 315/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8599 - accuracy: 0.7446\n",
      "Epoch 315: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8628 - accuracy: 0.7426 - val_loss: 2.9906 - val_accuracy: 0.5200\n",
      "Epoch 316/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9675 - accuracy: 0.7105\n",
      "Epoch 316: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9675 - accuracy: 0.7105 - val_loss: 2.8572 - val_accuracy: 0.5300\n",
      "Epoch 317/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8737 - accuracy: 0.7321\n",
      "Epoch 317: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8727 - accuracy: 0.7321 - val_loss: 2.8815 - val_accuracy: 0.5400\n",
      "Epoch 318/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9196 - accuracy: 0.7311\n",
      "Epoch 318: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9211 - accuracy: 0.7305 - val_loss: 2.9133 - val_accuracy: 0.5200\n",
      "Epoch 319/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.7103\n",
      "Epoch 319: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9152 - accuracy: 0.7111 - val_loss: 2.8651 - val_accuracy: 0.5400\n",
      "Epoch 320/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8643 - accuracy: 0.7360\n",
      "Epoch 320: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8607 - accuracy: 0.7379 - val_loss: 2.8227 - val_accuracy: 0.5200\n",
      "Epoch 321/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.8942 - accuracy: 0.7288\n",
      "Epoch 321: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.8933 - accuracy: 0.7279 - val_loss: 3.0255 - val_accuracy: 0.4900\n",
      "Epoch 322/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8494 - accuracy: 0.7374\n",
      "Epoch 322: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8494 - accuracy: 0.7374 - val_loss: 2.8516 - val_accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.8802 - accuracy: 0.7319\n",
      "Epoch 323: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8764 - accuracy: 0.7337 - val_loss: 3.0527 - val_accuracy: 0.4800\n",
      "Epoch 324/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.7474\n",
      "Epoch 324: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8074 - accuracy: 0.7474 - val_loss: 2.9777 - val_accuracy: 0.5400\n",
      "Epoch 325/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8689 - accuracy: 0.7432\n",
      "Epoch 325: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8689 - accuracy: 0.7432 - val_loss: 3.0106 - val_accuracy: 0.4900\n",
      "Epoch 326/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8950 - accuracy: 0.7219\n",
      "Epoch 326: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8978 - accuracy: 0.7221 - val_loss: 2.8887 - val_accuracy: 0.5000\n",
      "Epoch 327/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9042 - accuracy: 0.7161\n",
      "Epoch 327: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9044 - accuracy: 0.7158 - val_loss: 2.9829 - val_accuracy: 0.4600\n",
      "Epoch 328/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8741 - accuracy: 0.7263\n",
      "Epoch 328: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8741 - accuracy: 0.7263 - val_loss: 3.0282 - val_accuracy: 0.5200\n",
      "Epoch 329/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8309 - accuracy: 0.7532\n",
      "Epoch 329: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8359 - accuracy: 0.7511 - val_loss: 2.9936 - val_accuracy: 0.5100\n",
      "Epoch 330/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8239 - accuracy: 0.7608\n",
      "Epoch 330: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8302 - accuracy: 0.7611 - val_loss: 2.9585 - val_accuracy: 0.4800\n",
      "Epoch 331/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.8705 - accuracy: 0.7473\n",
      "Epoch 331: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8759 - accuracy: 0.7458 - val_loss: 2.9456 - val_accuracy: 0.5100\n",
      "Epoch 332/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8733 - accuracy: 0.7267\n",
      "Epoch 332: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8710 - accuracy: 0.7274 - val_loss: 2.9556 - val_accuracy: 0.5100\n",
      "Epoch 333/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8459 - accuracy: 0.7403\n",
      "Epoch 333: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8503 - accuracy: 0.7411 - val_loss: 2.7818 - val_accuracy: 0.5100\n",
      "Epoch 334/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8759 - accuracy: 0.7365\n",
      "Epoch 334: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8739 - accuracy: 0.7374 - val_loss: 2.9373 - val_accuracy: 0.5000\n",
      "Epoch 335/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8939 - accuracy: 0.7332\n",
      "Epoch 335: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8939 - accuracy: 0.7332 - val_loss: 2.9510 - val_accuracy: 0.5300\n",
      "Epoch 336/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8819 - accuracy: 0.7425\n",
      "Epoch 336: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8773 - accuracy: 0.7421 - val_loss: 3.0486 - val_accuracy: 0.5000\n",
      "Epoch 337/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8402 - accuracy: 0.7372\n",
      "Epoch 337: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8495 - accuracy: 0.7321 - val_loss: 2.8979 - val_accuracy: 0.5400\n",
      "Epoch 338/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8492 - accuracy: 0.7295\n",
      "Epoch 338: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8426 - accuracy: 0.7311 - val_loss: 2.9978 - val_accuracy: 0.5000\n",
      "Epoch 339/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8488 - accuracy: 0.7395\n",
      "Epoch 339: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8488 - accuracy: 0.7395 - val_loss: 3.2176 - val_accuracy: 0.4900\n",
      "Epoch 340/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8717 - accuracy: 0.7362\n",
      "Epoch 340: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8712 - accuracy: 0.7358 - val_loss: 3.2156 - val_accuracy: 0.4800\n",
      "Epoch 341/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8344 - accuracy: 0.7484\n",
      "Epoch 341: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8408 - accuracy: 0.7479 - val_loss: 3.2015 - val_accuracy: 0.5100\n",
      "Epoch 342/500\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.8501 - accuracy: 0.7619\n",
      "Epoch 342: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8541 - accuracy: 0.7595 - val_loss: 3.0528 - val_accuracy: 0.5100\n",
      "Epoch 343/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8185 - accuracy: 0.7545\n",
      "Epoch 343: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8369 - accuracy: 0.7500 - val_loss: 3.1656 - val_accuracy: 0.5300\n",
      "Epoch 344/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8965 - accuracy: 0.7309\n",
      "Epoch 344: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8980 - accuracy: 0.7316 - val_loss: 3.0818 - val_accuracy: 0.4800\n",
      "Epoch 345/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8761 - accuracy: 0.7379\n",
      "Epoch 345: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8761 - accuracy: 0.7379 - val_loss: 3.0309 - val_accuracy: 0.5000\n",
      "Epoch 346/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8591 - accuracy: 0.7430\n",
      "Epoch 346: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8594 - accuracy: 0.7416 - val_loss: 3.1219 - val_accuracy: 0.5100\n",
      "Epoch 347/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7979 - accuracy: 0.7484\n",
      "Epoch 347: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7979 - accuracy: 0.7484 - val_loss: 3.1797 - val_accuracy: 0.5100\n",
      "Epoch 348/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8436 - accuracy: 0.7383\n",
      "Epoch 348: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8412 - accuracy: 0.7389 - val_loss: 3.1033 - val_accuracy: 0.5100\n",
      "Epoch 349/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8134 - accuracy: 0.7489\n",
      "Epoch 349: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8101 - accuracy: 0.7500 - val_loss: 3.1006 - val_accuracy: 0.5200\n",
      "Epoch 350/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8407 - accuracy: 0.7442\n",
      "Epoch 350: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8445 - accuracy: 0.7432 - val_loss: 2.9896 - val_accuracy: 0.5300\n",
      "Epoch 351/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8429 - accuracy: 0.7511\n",
      "Epoch 351: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8463 - accuracy: 0.7495 - val_loss: 3.1119 - val_accuracy: 0.5000\n",
      "Epoch 352/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8932 - accuracy: 0.7394\n",
      "Epoch 352: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8925 - accuracy: 0.7389 - val_loss: 3.0933 - val_accuracy: 0.4900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8717 - accuracy: 0.7372\n",
      "Epoch 353: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8690 - accuracy: 0.7384 - val_loss: 3.0608 - val_accuracy: 0.5000\n",
      "Epoch 354/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.9154 - accuracy: 0.7299\n",
      "Epoch 354: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9124 - accuracy: 0.7305 - val_loss: 3.1539 - val_accuracy: 0.5000\n",
      "Epoch 355/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8947 - accuracy: 0.7371\n",
      "Epoch 355: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8918 - accuracy: 0.7384 - val_loss: 3.0979 - val_accuracy: 0.5300\n",
      "Epoch 356/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8949 - accuracy: 0.7389\n",
      "Epoch 356: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8919 - accuracy: 0.7389 - val_loss: 3.0182 - val_accuracy: 0.5200\n",
      "Epoch 357/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8171 - accuracy: 0.7455\n",
      "Epoch 357: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8076 - accuracy: 0.7505 - val_loss: 2.9941 - val_accuracy: 0.5300\n",
      "Epoch 358/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8364 - accuracy: 0.7426\n",
      "Epoch 358: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8364 - accuracy: 0.7426 - val_loss: 3.1918 - val_accuracy: 0.5200\n",
      "Epoch 359/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8094 - accuracy: 0.7511\n",
      "Epoch 359: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8094 - accuracy: 0.7511 - val_loss: 3.1082 - val_accuracy: 0.5100\n",
      "Epoch 360/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8691 - accuracy: 0.7436\n",
      "Epoch 360: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8538 - accuracy: 0.7416 - val_loss: 3.2076 - val_accuracy: 0.5100\n",
      "Epoch 361/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8568 - accuracy: 0.7331\n",
      "Epoch 361: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8607 - accuracy: 0.7321 - val_loss: 3.1256 - val_accuracy: 0.5100\n",
      "Epoch 362/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8650 - accuracy: 0.7288\n",
      "Epoch 362: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8623 - accuracy: 0.7300 - val_loss: 3.1558 - val_accuracy: 0.5000\n",
      "Epoch 363/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8080 - accuracy: 0.7489\n",
      "Epoch 363: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8057 - accuracy: 0.7495 - val_loss: 3.1428 - val_accuracy: 0.5100\n",
      "Epoch 364/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8696 - accuracy: 0.7232\n",
      "Epoch 364: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8751 - accuracy: 0.7284 - val_loss: 3.2078 - val_accuracy: 0.4800\n",
      "Epoch 365/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.7317\n",
      "Epoch 365: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9063 - accuracy: 0.7347 - val_loss: 2.9796 - val_accuracy: 0.5000\n",
      "Epoch 366/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8341 - accuracy: 0.7379\n",
      "Epoch 366: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8341 - accuracy: 0.7379 - val_loss: 3.0804 - val_accuracy: 0.5200\n",
      "Epoch 367/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7956 - accuracy: 0.7558\n",
      "Epoch 367: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7956 - accuracy: 0.7558 - val_loss: 3.0198 - val_accuracy: 0.5100\n",
      "Epoch 368/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8494 - accuracy: 0.7374\n",
      "Epoch 368: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8494 - accuracy: 0.7374 - val_loss: 2.9930 - val_accuracy: 0.5200\n",
      "Epoch 369/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8760 - accuracy: 0.7479\n",
      "Epoch 369: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8760 - accuracy: 0.7479 - val_loss: 2.9699 - val_accuracy: 0.5000\n",
      "Epoch 370/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8731 - accuracy: 0.7326\n",
      "Epoch 370: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8731 - accuracy: 0.7326 - val_loss: 3.0254 - val_accuracy: 0.5000\n",
      "Epoch 371/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8455 - accuracy: 0.7505\n",
      "Epoch 371: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8415 - accuracy: 0.7511 - val_loss: 2.8829 - val_accuracy: 0.5400\n",
      "Epoch 372/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7917 - accuracy: 0.7589\n",
      "Epoch 372: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7917 - accuracy: 0.7589 - val_loss: 3.0242 - val_accuracy: 0.5200\n",
      "Epoch 373/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.8568 - accuracy: 0.7440\n",
      "Epoch 373: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.8557 - accuracy: 0.7463 - val_loss: 2.8969 - val_accuracy: 0.5200\n",
      "Epoch 374/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8408 - accuracy: 0.7558\n",
      "Epoch 374: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8408 - accuracy: 0.7558 - val_loss: 2.9550 - val_accuracy: 0.5500\n",
      "Epoch 375/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8331 - accuracy: 0.7462\n",
      "Epoch 375: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8375 - accuracy: 0.7437 - val_loss: 3.0131 - val_accuracy: 0.5000\n",
      "Epoch 376/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8423 - accuracy: 0.7443\n",
      "Epoch 376: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8490 - accuracy: 0.7432 - val_loss: 3.0125 - val_accuracy: 0.5100\n",
      "Epoch 377/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8524 - accuracy: 0.7411\n",
      "Epoch 377: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8524 - accuracy: 0.7411 - val_loss: 3.0799 - val_accuracy: 0.5000\n",
      "Epoch 378/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.8407 - accuracy: 0.7434\n",
      "Epoch 378: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8431 - accuracy: 0.7426 - val_loss: 3.0813 - val_accuracy: 0.5200\n",
      "Epoch 379/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8163 - accuracy: 0.7548\n",
      "Epoch 379: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8167 - accuracy: 0.7547 - val_loss: 2.9795 - val_accuracy: 0.5000\n",
      "Epoch 380/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.7559\n",
      "Epoch 380: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7645 - accuracy: 0.7574 - val_loss: 3.1248 - val_accuracy: 0.5000\n",
      "Epoch 381/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8133 - accuracy: 0.7595\n",
      "Epoch 381: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8110 - accuracy: 0.7600 - val_loss: 3.1074 - val_accuracy: 0.5200\n",
      "Epoch 382/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8053 - accuracy: 0.7584\n",
      "Epoch 382: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8053 - accuracy: 0.7584 - val_loss: 3.1583 - val_accuracy: 0.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 383/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8358 - accuracy: 0.7489\n",
      "Epoch 383: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8358 - accuracy: 0.7489 - val_loss: 3.0973 - val_accuracy: 0.4900\n",
      "Epoch 384/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.7453\n",
      "Epoch 384: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8311 - accuracy: 0.7453 - val_loss: 3.0357 - val_accuracy: 0.5300\n",
      "Epoch 385/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8124 - accuracy: 0.7586\n",
      "Epoch 385: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8218 - accuracy: 0.7563 - val_loss: 3.0121 - val_accuracy: 0.5000\n",
      "Epoch 386/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.7600\n",
      "Epoch 386: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7879 - accuracy: 0.7600 - val_loss: 3.0641 - val_accuracy: 0.4900\n",
      "Epoch 387/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7828 - accuracy: 0.7597\n",
      "Epoch 387: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7866 - accuracy: 0.7584 - val_loss: 3.1397 - val_accuracy: 0.4900\n",
      "Epoch 388/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8139 - accuracy: 0.7613\n",
      "Epoch 388: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8141 - accuracy: 0.7605 - val_loss: 3.1011 - val_accuracy: 0.5200\n",
      "Epoch 389/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8198 - accuracy: 0.7551\n",
      "Epoch 389: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8260 - accuracy: 0.7547 - val_loss: 3.0394 - val_accuracy: 0.5200\n",
      "Epoch 390/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8274 - accuracy: 0.7505\n",
      "Epoch 390: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8296 - accuracy: 0.7500 - val_loss: 3.1013 - val_accuracy: 0.5000\n",
      "Epoch 391/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7574\n",
      "Epoch 391: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8040 - accuracy: 0.7574 - val_loss: 3.1547 - val_accuracy: 0.5100\n",
      "Epoch 392/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.8187 - accuracy: 0.7417\n",
      "Epoch 392: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.8048 - accuracy: 0.7463 - val_loss: 3.0891 - val_accuracy: 0.5500\n",
      "Epoch 393/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7451 - accuracy: 0.7723\n",
      "Epoch 393: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7574 - accuracy: 0.7684 - val_loss: 3.2250 - val_accuracy: 0.5000\n",
      "Epoch 394/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.8185 - accuracy: 0.7555\n",
      "Epoch 394: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8160 - accuracy: 0.7574 - val_loss: 3.0157 - val_accuracy: 0.4700\n",
      "Epoch 395/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.7547\n",
      "Epoch 395: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8223 - accuracy: 0.7547 - val_loss: 2.9851 - val_accuracy: 0.5200\n",
      "Epoch 396/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.7579\n",
      "Epoch 396: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7501 - accuracy: 0.7579 - val_loss: 3.1346 - val_accuracy: 0.5400\n",
      "Epoch 397/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7983 - accuracy: 0.7516\n",
      "Epoch 397: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7983 - accuracy: 0.7516 - val_loss: 3.1055 - val_accuracy: 0.5500\n",
      "Epoch 398/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.7621\n",
      "Epoch 398: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8024 - accuracy: 0.7621 - val_loss: 3.1851 - val_accuracy: 0.5200\n",
      "Epoch 399/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8258 - accuracy: 0.7449\n",
      "Epoch 399: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8326 - accuracy: 0.7405 - val_loss: 2.9169 - val_accuracy: 0.5500\n",
      "Epoch 400/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7826 - accuracy: 0.7627\n",
      "Epoch 400: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7804 - accuracy: 0.7637 - val_loss: 2.9993 - val_accuracy: 0.5400\n",
      "Epoch 401/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7953 - accuracy: 0.7548\n",
      "Epoch 401: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7939 - accuracy: 0.7547 - val_loss: 3.2006 - val_accuracy: 0.5000\n",
      "Epoch 402/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8993 - accuracy: 0.7368\n",
      "Epoch 402: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8993 - accuracy: 0.7368 - val_loss: 3.1839 - val_accuracy: 0.5400\n",
      "Epoch 403/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8027 - accuracy: 0.7468\n",
      "Epoch 403: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7996 - accuracy: 0.7479 - val_loss: 2.9755 - val_accuracy: 0.5500\n",
      "Epoch 404/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8188 - accuracy: 0.7500\n",
      "Epoch 404: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8208 - accuracy: 0.7484 - val_loss: 3.1088 - val_accuracy: 0.5500\n",
      "Epoch 405/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8336 - accuracy: 0.7452\n",
      "Epoch 405: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8404 - accuracy: 0.7458 - val_loss: 2.9663 - val_accuracy: 0.5100\n",
      "Epoch 406/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8428 - accuracy: 0.7570\n",
      "Epoch 406: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8431 - accuracy: 0.7563 - val_loss: 3.1050 - val_accuracy: 0.5400\n",
      "Epoch 407/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8259 - accuracy: 0.7441\n",
      "Epoch 407: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8285 - accuracy: 0.7432 - val_loss: 3.1667 - val_accuracy: 0.5200\n",
      "Epoch 408/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8119 - accuracy: 0.7489\n",
      "Epoch 408: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8113 - accuracy: 0.7495 - val_loss: 3.2004 - val_accuracy: 0.5100\n",
      "Epoch 409/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8000 - accuracy: 0.7579\n",
      "Epoch 409: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8000 - accuracy: 0.7579 - val_loss: 3.1191 - val_accuracy: 0.5200\n",
      "Epoch 410/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8021 - accuracy: 0.7672\n",
      "Epoch 410: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7825 - accuracy: 0.7674 - val_loss: 2.9917 - val_accuracy: 0.5400\n",
      "Epoch 411/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7744 - accuracy: 0.7683\n",
      "Epoch 411: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7742 - accuracy: 0.7674 - val_loss: 3.1565 - val_accuracy: 0.4900\n",
      "Epoch 412/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7872 - accuracy: 0.7629\n",
      "Epoch 412: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7863 - accuracy: 0.7637 - val_loss: 3.0346 - val_accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8149 - accuracy: 0.7632\n",
      "Epoch 413: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8156 - accuracy: 0.7626 - val_loss: 3.2703 - val_accuracy: 0.5200\n",
      "Epoch 414/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8308 - accuracy: 0.7495\n",
      "Epoch 414: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8338 - accuracy: 0.7495 - val_loss: 3.0476 - val_accuracy: 0.5400\n",
      "Epoch 415/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7966 - accuracy: 0.7632\n",
      "Epoch 415: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7966 - accuracy: 0.7632 - val_loss: 3.0814 - val_accuracy: 0.5500\n",
      "Epoch 416/500\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.7779 - accuracy: 0.7686\n",
      "Epoch 416: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.7901 - accuracy: 0.7647 - val_loss: 3.2302 - val_accuracy: 0.5200\n",
      "Epoch 417/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7563\n",
      "Epoch 417: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8172 - accuracy: 0.7563 - val_loss: 3.2672 - val_accuracy: 0.5300\n",
      "Epoch 418/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7756 - accuracy: 0.7666\n",
      "Epoch 418: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7749 - accuracy: 0.7716 - val_loss: 3.1499 - val_accuracy: 0.5300\n",
      "Epoch 419/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7835 - accuracy: 0.7653\n",
      "Epoch 419: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7835 - accuracy: 0.7653 - val_loss: 3.0690 - val_accuracy: 0.5200\n",
      "Epoch 420/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8350 - accuracy: 0.7463\n",
      "Epoch 420: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8350 - accuracy: 0.7463 - val_loss: 3.3101 - val_accuracy: 0.5300\n",
      "Epoch 421/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8413 - accuracy: 0.7505\n",
      "Epoch 421: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8413 - accuracy: 0.7505 - val_loss: 3.3828 - val_accuracy: 0.5300\n",
      "Epoch 422/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7207 - accuracy: 0.7730\n",
      "Epoch 422: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7220 - accuracy: 0.7726 - val_loss: 3.2831 - val_accuracy: 0.5400\n",
      "Epoch 423/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8332 - accuracy: 0.7621\n",
      "Epoch 423: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8304 - accuracy: 0.7611 - val_loss: 3.3672 - val_accuracy: 0.5300\n",
      "Epoch 424/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7506 - accuracy: 0.7828\n",
      "Epoch 424: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7494 - accuracy: 0.7832 - val_loss: 3.1109 - val_accuracy: 0.5300\n",
      "Epoch 425/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7834 - accuracy: 0.7737\n",
      "Epoch 425: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7834 - accuracy: 0.7737 - val_loss: 3.1403 - val_accuracy: 0.5000\n",
      "Epoch 426/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7943 - accuracy: 0.7600\n",
      "Epoch 426: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7943 - accuracy: 0.7600 - val_loss: 3.3196 - val_accuracy: 0.5400\n",
      "Epoch 427/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8437 - accuracy: 0.7526\n",
      "Epoch 427: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8478 - accuracy: 0.7516 - val_loss: 3.2762 - val_accuracy: 0.5200\n",
      "Epoch 428/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7722 - accuracy: 0.7608\n",
      "Epoch 428: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7777 - accuracy: 0.7595 - val_loss: 3.2541 - val_accuracy: 0.5100\n",
      "Epoch 429/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7599 - accuracy: 0.7785\n",
      "Epoch 429: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7553 - accuracy: 0.7800 - val_loss: 3.2645 - val_accuracy: 0.5000\n",
      "Epoch 430/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8047 - accuracy: 0.7597\n",
      "Epoch 430: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7996 - accuracy: 0.7616 - val_loss: 3.1959 - val_accuracy: 0.4900\n",
      "Epoch 431/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7990 - accuracy: 0.7590\n",
      "Epoch 431: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7980 - accuracy: 0.7600 - val_loss: 3.2578 - val_accuracy: 0.4900\n",
      "Epoch 432/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7931 - accuracy: 0.7548\n",
      "Epoch 432: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7957 - accuracy: 0.7532 - val_loss: 3.0797 - val_accuracy: 0.4700\n",
      "Epoch 433/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7471 - accuracy: 0.7656\n",
      "Epoch 433: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7411 - accuracy: 0.7674 - val_loss: 3.1850 - val_accuracy: 0.4900\n",
      "Epoch 434/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7655 - accuracy: 0.7698\n",
      "Epoch 434: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7620 - accuracy: 0.7658 - val_loss: 3.2295 - val_accuracy: 0.5200\n",
      "Epoch 435/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7201 - accuracy: 0.7748\n",
      "Epoch 435: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7152 - accuracy: 0.7763 - val_loss: 3.3774 - val_accuracy: 0.5200\n",
      "Epoch 436/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7765 - accuracy: 0.7570\n",
      "Epoch 436: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7714 - accuracy: 0.7568 - val_loss: 3.3365 - val_accuracy: 0.4900\n",
      "Epoch 437/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7737 - accuracy: 0.7642\n",
      "Epoch 437: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7737 - accuracy: 0.7642 - val_loss: 3.2031 - val_accuracy: 0.5300\n",
      "Epoch 438/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7823 - accuracy: 0.7564\n",
      "Epoch 438: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7885 - accuracy: 0.7563 - val_loss: 3.3794 - val_accuracy: 0.4900\n",
      "Epoch 439/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.7616\n",
      "Epoch 439: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7854 - accuracy: 0.7616 - val_loss: 3.3128 - val_accuracy: 0.5200\n",
      "Epoch 440/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.7654\n",
      "Epoch 440: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7692 - accuracy: 0.7653 - val_loss: 3.2260 - val_accuracy: 0.5100\n",
      "Epoch 441/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.7837\n",
      "Epoch 441: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7245 - accuracy: 0.7837 - val_loss: 3.2136 - val_accuracy: 0.5000\n",
      "Epoch 442/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7565 - accuracy: 0.7747\n",
      "Epoch 442: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7565 - accuracy: 0.7747 - val_loss: 3.2552 - val_accuracy: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7639 - accuracy: 0.7664\n",
      "Epoch 443: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.7689 - val_loss: 3.3863 - val_accuracy: 0.5200\n",
      "Epoch 444/500\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.7951 - accuracy: 0.7647\n",
      "Epoch 444: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.7844 - accuracy: 0.7663 - val_loss: 3.2487 - val_accuracy: 0.5500\n",
      "Epoch 445/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.7716\n",
      "Epoch 445: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7510 - accuracy: 0.7716 - val_loss: 3.3527 - val_accuracy: 0.4900\n",
      "Epoch 446/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7673 - accuracy: 0.7730\n",
      "Epoch 446: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7771 - accuracy: 0.7711 - val_loss: 3.2390 - val_accuracy: 0.5000\n",
      "Epoch 447/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7133 - accuracy: 0.7832\n",
      "Epoch 447: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7292 - accuracy: 0.7747 - val_loss: 3.3132 - val_accuracy: 0.5200\n",
      "Epoch 448/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.7659\n",
      "Epoch 448: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7894 - accuracy: 0.7647 - val_loss: 3.3082 - val_accuracy: 0.5000\n",
      "Epoch 449/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7798 - accuracy: 0.7656\n",
      "Epoch 449: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7798 - accuracy: 0.7653 - val_loss: 3.2373 - val_accuracy: 0.5400\n",
      "Epoch 450/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8429 - accuracy: 0.7584\n",
      "Epoch 450: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8308 - accuracy: 0.7595 - val_loss: 3.1382 - val_accuracy: 0.5300\n",
      "Epoch 451/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7708 - accuracy: 0.7744\n",
      "Epoch 451: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7720 - accuracy: 0.7747 - val_loss: 3.3400 - val_accuracy: 0.5300\n",
      "Epoch 452/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7648 - accuracy: 0.7679\n",
      "Epoch 452: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7648 - accuracy: 0.7679 - val_loss: 3.3046 - val_accuracy: 0.5300\n",
      "Epoch 453/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7881 - accuracy: 0.7656\n",
      "Epoch 453: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7908 - accuracy: 0.7647 - val_loss: 3.3662 - val_accuracy: 0.5300\n",
      "Epoch 454/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7568 - accuracy: 0.7742\n",
      "Epoch 454: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7568 - accuracy: 0.7742 - val_loss: 3.1707 - val_accuracy: 0.5000\n",
      "Epoch 455/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7786 - accuracy: 0.7700\n",
      "Epoch 455: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7786 - accuracy: 0.7700 - val_loss: 3.1951 - val_accuracy: 0.5200\n",
      "Epoch 456/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7807\n",
      "Epoch 456: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7410 - accuracy: 0.7811 - val_loss: 3.3603 - val_accuracy: 0.5500\n",
      "Epoch 457/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7707 - accuracy: 0.7711\n",
      "Epoch 457: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7707 - accuracy: 0.7711 - val_loss: 3.0932 - val_accuracy: 0.5600\n",
      "Epoch 458/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.8220 - accuracy: 0.7532\n",
      "Epoch 458: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8056 - accuracy: 0.7616 - val_loss: 3.2313 - val_accuracy: 0.5200\n",
      "Epoch 459/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7843 - accuracy: 0.7667\n",
      "Epoch 459: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7863 - accuracy: 0.7663 - val_loss: 3.4522 - val_accuracy: 0.5000\n",
      "Epoch 460/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7704 - accuracy: 0.7691\n",
      "Epoch 460: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7679 - accuracy: 0.7695 - val_loss: 3.3729 - val_accuracy: 0.5100\n",
      "Epoch 461/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8086 - accuracy: 0.7442\n",
      "Epoch 461: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8086 - accuracy: 0.7442 - val_loss: 3.1728 - val_accuracy: 0.5400\n",
      "Epoch 462/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8055 - accuracy: 0.7579\n",
      "Epoch 462: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8053 - accuracy: 0.7584 - val_loss: 3.1052 - val_accuracy: 0.5400\n",
      "Epoch 463/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7820 - accuracy: 0.7691\n",
      "Epoch 463: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7678 - accuracy: 0.7747 - val_loss: 3.2710 - val_accuracy: 0.5200\n",
      "Epoch 464/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7368 - accuracy: 0.7683\n",
      "Epoch 464: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7350 - accuracy: 0.7684 - val_loss: 3.3893 - val_accuracy: 0.5200\n",
      "Epoch 465/500\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.7841 - accuracy: 0.7720\n",
      "Epoch 465: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.7798 - accuracy: 0.7721 - val_loss: 3.3273 - val_accuracy: 0.5300\n",
      "Epoch 466/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7495 - accuracy: 0.7680\n",
      "Epoch 466: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7524 - accuracy: 0.7663 - val_loss: 3.2697 - val_accuracy: 0.5400\n",
      "Epoch 467/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.7711\n",
      "Epoch 467: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7735 - accuracy: 0.7711 - val_loss: 3.2801 - val_accuracy: 0.5700\n",
      "Epoch 468/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7507 - accuracy: 0.7741\n",
      "Epoch 468: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7554 - accuracy: 0.7716 - val_loss: 3.3353 - val_accuracy: 0.5100\n",
      "Epoch 469/500\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.7030 - accuracy: 0.7812\n",
      "Epoch 469: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.7059 - accuracy: 0.7816 - val_loss: 3.4008 - val_accuracy: 0.5100\n",
      "Epoch 470/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7249 - accuracy: 0.7728\n",
      "Epoch 470: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7247 - accuracy: 0.7732 - val_loss: 3.2652 - val_accuracy: 0.5500\n",
      "Epoch 471/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7164 - accuracy: 0.7812\n",
      "Epoch 471: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7196 - accuracy: 0.7795 - val_loss: 3.1267 - val_accuracy: 0.5800\n",
      "Epoch 472/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7744 - accuracy: 0.7791\n",
      "Epoch 472: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7707 - accuracy: 0.7800 - val_loss: 3.1173 - val_accuracy: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7690 - accuracy: 0.7707\n",
      "Epoch 473: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7731 - accuracy: 0.7700 - val_loss: 3.2671 - val_accuracy: 0.5300\n",
      "Epoch 474/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7754\n",
      "Epoch 474: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7740 - accuracy: 0.7758 - val_loss: 3.3699 - val_accuracy: 0.5100\n",
      "Epoch 475/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7470 - accuracy: 0.7732\n",
      "Epoch 475: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7470 - accuracy: 0.7732 - val_loss: 3.1739 - val_accuracy: 0.5300\n",
      "Epoch 476/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7842\n",
      "Epoch 476: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.6967 - accuracy: 0.7842 - val_loss: 3.3085 - val_accuracy: 0.5300\n",
      "Epoch 477/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7645 - accuracy: 0.7769\n",
      "Epoch 477: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7665 - accuracy: 0.7758 - val_loss: 3.3148 - val_accuracy: 0.5400\n",
      "Epoch 478/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7329 - accuracy: 0.7881\n",
      "Epoch 478: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7333 - accuracy: 0.7879 - val_loss: 3.2820 - val_accuracy: 0.5000\n",
      "Epoch 479/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8373 - accuracy: 0.7558\n",
      "Epoch 479: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8356 - accuracy: 0.7563 - val_loss: 3.3498 - val_accuracy: 0.5100\n",
      "Epoch 480/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7411 - accuracy: 0.7737\n",
      "Epoch 480: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7411 - accuracy: 0.7737 - val_loss: 3.5832 - val_accuracy: 0.4800\n",
      "Epoch 481/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7238 - accuracy: 0.7758\n",
      "Epoch 481: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7171 - accuracy: 0.7774 - val_loss: 3.2883 - val_accuracy: 0.5000\n",
      "Epoch 482/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7505 - accuracy: 0.7651\n",
      "Epoch 482: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7471 - accuracy: 0.7674 - val_loss: 3.3876 - val_accuracy: 0.5300\n",
      "Epoch 483/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.7758\n",
      "Epoch 483: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7175 - accuracy: 0.7758 - val_loss: 3.3994 - val_accuracy: 0.5200\n",
      "Epoch 484/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7197 - accuracy: 0.7744\n",
      "Epoch 484: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7207 - accuracy: 0.7747 - val_loss: 3.6020 - val_accuracy: 0.5400\n",
      "Epoch 485/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7261 - accuracy: 0.7818\n",
      "Epoch 485: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7293 - accuracy: 0.7811 - val_loss: 3.4346 - val_accuracy: 0.5400\n",
      "Epoch 486/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.7689\n",
      "Epoch 486: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7420 - accuracy: 0.7689 - val_loss: 3.2077 - val_accuracy: 0.5300\n",
      "Epoch 487/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7718 - accuracy: 0.7710\n",
      "Epoch 487: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7673 - accuracy: 0.7721 - val_loss: 3.2066 - val_accuracy: 0.5300\n",
      "Epoch 488/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7084 - accuracy: 0.7953\n",
      "Epoch 488: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7084 - accuracy: 0.7953 - val_loss: 3.3055 - val_accuracy: 0.4900\n",
      "Epoch 489/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7464 - accuracy: 0.7892\n",
      "Epoch 489: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7447 - accuracy: 0.7889 - val_loss: 3.3991 - val_accuracy: 0.4600\n",
      "Epoch 490/500\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.7850\n",
      "Epoch 490: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7026 - accuracy: 0.7853 - val_loss: 3.5376 - val_accuracy: 0.4600\n",
      "Epoch 491/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.8007 - accuracy: 0.7621\n",
      "Epoch 491: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.8007 - accuracy: 0.7621 - val_loss: 3.1427 - val_accuracy: 0.5100\n",
      "Epoch 492/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7761 - accuracy: 0.7819\n",
      "Epoch 492: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7631 - accuracy: 0.7832 - val_loss: 3.3605 - val_accuracy: 0.5200\n",
      "Epoch 493/500\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.7346 - accuracy: 0.7701\n",
      "Epoch 493: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7299 - accuracy: 0.7695 - val_loss: 3.3975 - val_accuracy: 0.5100\n",
      "Epoch 494/500\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.7737 - accuracy: 0.7730\n",
      "Epoch 494: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7714 - accuracy: 0.7726 - val_loss: 3.2773 - val_accuracy: 0.5200\n",
      "Epoch 495/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7412 - accuracy: 0.7795\n",
      "Epoch 495: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7412 - accuracy: 0.7795 - val_loss: 3.3719 - val_accuracy: 0.5300\n",
      "Epoch 496/500\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.7091 - accuracy: 0.7864\n",
      "Epoch 496: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.7900 - val_loss: 3.3642 - val_accuracy: 0.5300\n",
      "Epoch 497/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.7837\n",
      "Epoch 497: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.6959 - accuracy: 0.7837 - val_loss: 3.5077 - val_accuracy: 0.5300\n",
      "Epoch 498/500\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7305 - accuracy: 0.7872\n",
      "Epoch 498: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7260 - accuracy: 0.7889 - val_loss: 3.4752 - val_accuracy: 0.4900\n",
      "Epoch 499/500\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.7329 - accuracy: 0.7767\n",
      "Epoch 499: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7336 - accuracy: 0.7779 - val_loss: 3.5879 - val_accuracy: 0.4900\n",
      "Epoch 500/500\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.7942\n",
      "Epoch 500: val_loss did not improve from 2.22695\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.7023 - accuracy: 0.7942 - val_loss: 3.4847 - val_accuracy: 0.4900\n"
     ]
    }
   ],
   "source": [
    "X_test, T_test = Xorig[1900:], Torig[1900:]\n",
    "\n",
    "# Run inference on CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    Xtrain, Ttrain = tf.convert_to_tensor(Xorig[:1900]), tf.convert_to_tensor(Torig[:1900])\n",
    "    \n",
    "# # Run training on GPU\n",
    "# with tf.device('/gpu:0'):\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./audio_classification.hdf5', \n",
    "                           verbose=3, save_best_only=True)\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)\n",
    "\n",
    "batch_size = 100\n",
    "steps_per_epoch = Xtrain.shape[0] // batch_size\n",
    "history = model.fit(Xtrain, Ttrain, \n",
    "          epochs=500, \n",
    "          validation_data=(X_test, T_test), \n",
    "#               steps_per_epoch=steps_per_epoch, \n",
    "#               batch_size = batch_size,\n",
    "          callbacks=[checkpointer]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c4b7902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label= 'train_loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4bbf7",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a672056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "Predited 44 Target 9\n",
      "Predited 49 Target 49\n",
      "Predited 32 Target 28\n",
      "Predited 34 Target 29\n",
      "Predited 43 Target 43\n",
      "Predited 34 Target 28\n",
      "Predited 40 Target 40\n",
      "Predited 30 Target 43\n",
      "Predited 21 Target 2\n",
      "Predited 28 Target 28\n",
      "Predited 48 Target 48\n",
      "Predited 2 Target 49\n",
      "Predited 31 Target 15\n",
      "Predited 0 Target 0\n",
      "Predited 45 Target 44\n",
      "Predited 45 Target 45\n",
      "Predited 27 Target 17\n",
      "Predited 34 Target 38\n",
      "Predited 31 Target 24\n",
      "Predited 38 Target 15\n",
      "Predited 11 Target 11\n",
      "Predited 44 Target 44\n",
      "Predited 11 Target 11\n",
      "Predited 34 Target 17\n",
      "Predited 7 Target 35\n",
      "Predited 12 Target 31\n",
      "Predited 0 Target 0\n",
      "Predited 1 Target 5\n",
      "Predited 36 Target 36\n",
      "Predited 7 Target 7\n",
      "Predited 43 Target 2\n",
      "Predited 41 Target 41\n",
      "Predited 5 Target 5\n",
      "Predited 23 Target 32\n",
      "Predited 10 Target 10\n",
      "Predited 47 Target 47\n",
      "Predited 44 Target 44\n",
      "Predited 29 Target 21\n",
      "Predited 21 Target 21\n",
      "Predited 12 Target 12\n",
      "Predited 21 Target 43\n",
      "Predited 25 Target 33\n",
      "Predited 14 Target 14\n",
      "Predited 21 Target 21\n",
      "Predited 20 Target 20\n",
      "Predited 45 Target 40\n",
      "Predited 18 Target 18\n",
      "Predited 6 Target 6\n",
      "Predited 6 Target 0\n",
      "Predited 5 Target 0\n",
      "Predited 9 Target 3\n",
      "Predited 24 Target 21\n",
      "Predited 39 Target 21\n",
      "Predited 43 Target 43\n",
      "Predited 22 Target 22\n",
      "Predited 16 Target 16\n",
      "Predited 4 Target 13\n",
      "Predited 30 Target 30\n",
      "Predited 19 Target 19\n",
      "Predited 17 Target 27\n",
      "Predited 35 Target 44\n",
      "Predited 8 Target 8\n",
      "Predited 30 Target 30\n",
      "Predited 9 Target 33\n",
      "Predited 7 Target 13\n",
      "Predited 21 Target 24\n",
      "Predited 21 Target 24\n",
      "Predited 47 Target 47\n",
      "Predited 10 Target 41\n",
      "Predited 4 Target 13\n",
      "Predited 24 Target 21\n",
      "Predited 18 Target 33\n",
      "Predited 33 Target 33\n",
      "Predited 12 Target 13\n",
      "Predited 4 Target 46\n",
      "Predited 9 Target 1\n",
      "Predited 31 Target 27\n",
      "Predited 25 Target 22\n",
      "Predited 1 Target 1\n",
      "Predited 6 Target 6\n",
      "Predited 6 Target 6\n",
      "Predited 9 Target 9\n",
      "Predited 38 Target 24\n",
      "Predited 42 Target 44\n",
      "Predited 2 Target 2\n",
      "Predited 37 Target 37\n",
      "Predited 39 Target 39\n",
      "Predited 37 Target 37\n",
      "Predited 30 Target 29\n",
      "Predited 46 Target 23\n",
      "Predited 30 Target 38\n",
      "Predited 45 Target 45\n",
      "Predited 5 Target 5\n",
      "Predited 18 Target 10\n",
      "Predited 35 Target 35\n",
      "Predited 47 Target 47\n",
      "Predited 46 Target 46\n",
      "Predited 23 Target 23\n",
      "Predited 7 Target 7\n",
      "Predited 36 Target 44\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict(X_test) \n",
    "classes_x = np.argmax(predict_x,axis=1)\n",
    "for i in range(len(classes_x)) :\n",
    "    print(f'Predited {classes_x[i]} Target {T_test[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df50572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
