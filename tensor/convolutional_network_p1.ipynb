{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f0f16e",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The problem we will consider here is classifying 10 different everyday objects. The dataset we will use is built into tensorflow and called the [**CIFAR Image Dataset.**](https://www.cs.toronto.edu/~kriz/cifar.html) It contains 60,000 32x32 color images with 6000 images of each class. \n",
    "\n",
    "The labels in this dataset are the following:\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277a0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e608ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOAD AND SPLIT DATASET\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f2666e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef5e0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at a one image\n",
    "IMG_INDEX = 7  # change this to look at other images\n",
    "\n",
    "plt.imshow(train_images[IMG_INDEX])\n",
    "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7067dc71",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few denesly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n",
    "\n",
    "We will start by building the **Convolutional Base**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5247b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))) # 32 = amount of filters, (3, 3) = size of those filters\n",
    "#relu = rectify linear unit....after dot product we will apply this activation function\n",
    "model.add(layers.MaxPooling2D((2, 2)))  # this is to reduce the size of the size of the filters....not necessary\n",
    "# 2x2 sample size with a stride of 2\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1df848",
   "metadata": {},
   "source": [
    "**Layer 1**\n",
    "\n",
    "The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3x3 over our input data. We will also apply the activation function relu to the output of each convolution operation.\n",
    "\n",
    "**Layer 2**\n",
    "\n",
    "This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
    "\n",
    "**Other Layers**\n",
    "\n",
    "The next set of layers do very similar things but take as input the feature map from the previous layer. They also increase the frequency of filters from 32 to 64. We can do this as our data shrinks in spacial dimensions as it passed through the layers, meaning we can afford (computationally) to add more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c86e063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,320\n",
      "Trainable params: 56,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6f8b1",
   "metadata": {},
   "source": [
    "## Adding Dense Layers\n",
    "So far, we have just completed the **convolutional base**. Now we need to take these extracted features and add a way to classify them. This is why we add the following layers to our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01e46aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fe5eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,570\n",
      "Trainable params: 122,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee611a0",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will train and compile the model using the recommended hyper paramaters from tensorflow.\n",
    "\n",
    "*Note: This will take much longer than previous models!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955ec37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1563/1563 [==============================] - 31s 19ms/step - loss: 1.5138 - accuracy: 0.4510 - val_loss: 1.2754 - val_accuracy: 0.5458\n",
      "Epoch 2/4\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1369 - accuracy: 0.5977 - val_loss: 1.0493 - val_accuracy: 0.6336\n",
      "Epoch 3/4\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 0.9970 - accuracy: 0.6514 - val_loss: 0.9650 - val_accuracy: 0.6628\n",
      "Epoch 4/4\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.9023 - accuracy: 0.6833 - val_loss: 0.9283 - val_accuracy: 0.6687\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=4, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691589c5",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We can determine how well the model performed by looking at it's performance on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30fa8f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 0.9283 - accuracy: 0.6687 - 2s/epoch - 5ms/step\n",
      "0.6686999797821045\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8956",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "To avoid overfitting and create a larger dataset from a smaller one we can use a technique called data augmentation. This is simply performing random transofrmations on our images so that our model can generalize better. These transformations can be things like compressions, rotations, stretches and even color changes. \n",
    "\n",
    "Fortunately, keras can help us do this. Look at the code below to an example of data augmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81792b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'img_to_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3996/2148921265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# pick an image to transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# convert image to numpy arry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# reshape image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'img_to_array'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# creates a data generator object that transforms images\n",
    "datagen = ImageDataGenerator(\n",
    "rotation_range=40,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "shear_range=0.2,\n",
    "zoom_range=0.2,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "\n",
    "# pick an image to transform\n",
    "test_img = train_images[20]\n",
    "img = image.img_to_array(test_img)  # convert image to numpy arry\n",
    "img = img.reshape((1,) + img.shape)  # reshape image\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n",
    "    plt.figure(i)\n",
    "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
    "    i += 1\n",
    "    if i > 4:  # show 4 images\n",
    "        break\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a81905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
